{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "d9d363e889bac6e082d872d6c6a952a67371d63acf869877e6b992428fa21dbf"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 1.文本切分"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "一段文本或一个文本文件具有几个组成部分，包括可以进一步细分为从句、短语和单词的语句。最流行的文本切分技术包括句子切分和词语切分，用于将文本语料库分解成句子，并将每个句子分解成单词。因此，文本切分可以定义为将文本数据分 解或拆分为具有更小且有意义的成分（即标识）的过程。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1.1 句子切分"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "句子切分(sentence tokenization)是将文本语料库分解为句子的过程，这些句子是组成语料库的第一季切分结果，这个过程也叫句子分割。我们尝试将文本分割成有意义的句子。执行句子切分有多种技术，基本技术包括在句子之间寻找特定的分隔符，例如句号(.)、换行符(\\n)或者分号(;)。我们将使用NLTK框架进行切分，该框架提供用于执行句子切分的各种接口：                                \n",
    "   - sent_tokenize\n",
    "   - PunktSentence Tokenization\n",
    "   - Regexp Tokenization\n",
    "   - 预训练的句子切分模型                     \n",
    "\n",
    "我们使用NLTK中古腾堡(Gutenberg)语料库："
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from pprint import pprint"
   ]
  },
  {
   "source": [
    "注：print()和pprint()都是python的打印模块，功能基本一样，唯一的区别就是pprint()模块打印出来的数据结构更加完整，每行为一个数据结构，更加方便阅读打印输出结果。特别是对于特别长的数据打印，print()输出结果都在一行，不方便查看，而pprint()采用分行打印输出，所以对于数据结构比较复杂、数据长度较长的数据，适合采用pprint()打印方式。当然，一般情况多数采用print()。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\Leo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\gutenberg.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# 下载古登堡数据集\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载文本集\n",
    "alice = gutenberg.raw(fileids=\"carroll-alice.txt\")\n",
    "sample_text = 'we will discuss briefly about the basic syntax, structure and design philosophies. There is a defined hierarchical syntax for Python code which you should remember when writing code! Python is a really powerful programming language!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "144395\n[Alice's Adventures in Wonderland by Lewis Carroll 1865]\n\nCHAPTER I. Down the Rabbit-Hole\n\nAlice was\n"
     ]
    }
   ],
   "source": [
    "# 查看Alice in Wonderland 语料库的长度\n",
    "print(len(alice))\n",
    "# 查看Alice in Wonderland 语料库前100个字符\n",
    "print(alice[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total sentences in sample_text: 3\nsample text sentences :-\n['we will discuss briefly about the basic syntax, structure and design philosophies.', 'There is a defined hierarchical syntax for Python code which you should remember when writing code!', 'Python is a really powerful programming language!']\n\nTotal sentences in alice: 1625\nFirst 5 sentences in alice :-\n[\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I.\", \"Down the Rabbit-Hole\\n\\nAlice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to do: once or twice she had peeped into the\\nbook her sister was reading, but it had no pictures or conversations in\\nit, 'and what is the use of a book,' thought Alice 'without pictures or\\nconversation?'\", 'So she was considering in her own mind (as well as she could, for the\\nhot day made her feel very sleepy and stupid), whether the pleasure\\nof making a daisy-chain would be worth the trouble of getting up and\\npicking the daisies, when suddenly a White Rabbit with pink eyes ran\\nclose by her.', \"There was nothing so VERY remarkable in that; nor did Alice think it so\\nVERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\", 'Oh dear!']\n"
     ]
    }
   ],
   "source": [
    "# nltk.sent_tokenize是nltk默认的句子切分函数\n",
    "default_st = nltk.sent_tokenize\n",
    "alice_sentences = default_st(text=alice)\n",
    "sample_sentences = default_st(text=sample_text)\n",
    "print('Total sentences in sample_text:',len(sample_sentences))\n",
    "print('sample text sentences :-')\n",
    "print(sample_sentences)\n",
    "print('\\nTotal sentences in alice:',len(alice_sentences))\n",
    "print('First 5 sentences in alice :-')\n",
    "print(alice_sentences[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[nltk_data] Downloading package europarl_raw to\n",
      "[nltk_data]     C:\\Users\\Leo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\europarl_raw.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# 下面来看看德国文档(下载德语文本语料库)\n",
    "nltk.download('europarl_raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "157171\n \nWiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sit\nTrue\n \nWiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen , wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe , daß Sie schöne Ferien hatten .\nWie Sie feststellen konnten , ist der gefürchtete \" Millenium-Bug \" nicht eingetreten .\nDoch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden .\nIm Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser Sitzungsperiode in den nächsten Tagen .\nHeute möchte ich Sie bitten - das ist auch der Wunsch einiger Kolleginnen und Kollegen - , allen Opfern der Stürme , insbesondere in den verschiedenen Ländern der Europäischen Union , in einer Schweigeminute zu gedenken .\n"
     ]
    }
   ],
   "source": [
    "# 下面对德语文本进行句子切分\n",
    "from nltk.corpus import europarl_raw\n",
    "german_text = europarl_raw.german.raw(fileids='ep-00-01-17.de')\n",
    "# 文本的总句子数\n",
    "print(len(german_text))\n",
    "# 德语文本前100个字符\n",
    "print(german_text[0:100])\n",
    "# 使用默认的sent_tokenize 切分器\n",
    "german_sentences_def = default_st(text=german_text,language='german')\n",
    "# 使用从nltk源加载的预训练的德语切分器\n",
    "german_tokenizer = nltk.data.load(resource_url='tokenizers/punkt/german.pickle')\n",
    "german_sentences = german_tokenizer.tokenize(german_text)\n",
    "# 判断默认方式下的切分与预训练的德语切分器的结果是否一致\n",
    "print(german_sentences_def == german_sentences)\n",
    "# 输出预训练切分句子的前五句\n",
    "for sent in german_sentences[0:5]:\n",
    "    print(sent)"
   ]
  },
  {
   "source": [
    "我们得出的结论是：默认切分器和预训练切分器的结果完全一致"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "最后再介绍一种基于正则表达式的句子切分方式，使用RegexpTokenizer类的示例切分为句子："
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['we will discuss briefly about the basic syntax, structure and design philosophies.', 'There is a defined hierarchical syntax for Python code which you should remember when writing code!', 'Python is a really powerful programming language!']\n"
     ]
    }
   ],
   "source": [
    "SENTENCE_TOKENS_PATTERN = r'(?<!\\w\\.\\w)(?<![A-Z][a-z]\\.)(?<![A-Z]\\.)(?<=\\.|\\?|\\!)\\s'\n",
    "regex_st = nltk.tokenize.RegexpTokenizer(pattern=SENTENCE_TOKENS_PATTERN,gaps=True)\n",
    "sample_sentences = regex_st.tokenize(sample_text)\n",
    "print(sample_sentences)"
   ]
  },
  {
   "source": [
    "## 1.2 词语切分"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "词语切分（word tokenization）是将句子分解或分割成其组成单词的过程。句子是单词的集合，通过词语切分，在本质上，将一个句子分割成单词列表，该单词列表又可以重建句子。词语切分在很多过程中都是非常重要的，特别是在文本清洗和规范化时，诸如词干提取 和词形还原这类基于词干、标识信息的操作会在每个单词上实施。与句子切分类似，nltk为词语切分提供了各种有用的接口：                           \n",
    "   - word_tokenize\n",
    "   - TreebankWordTokenizer\n",
    "   - RegexpTokenizer\n",
    "   - 从RegexpTokenizer继承的切分器"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['The', 'brown', 'fox', 'was', \"n't\", 'that', 'quick', 'and', 'he', 'could', \"n't\", 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "# 我们使用例句作为默认切分器的输入，并切分成词语\n",
    "sentence = \"The brown fox wasn't that quick and he couldn't win the race\"\n",
    "default_wt = nltk.word_tokenize\n",
    "words = default_wt(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['The', 'brown', 'fox', 'was', \"n't\", 'that', 'quick', 'and', 'he', 'could', \"n't\", 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "# 使用TreebankWordTokenizer类切分句子称为单词\n",
    "treebank_wt = nltk.TreebankWordTokenizer()\n",
    "words = treebank_wt.tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# 2.文本规范化"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "文本规范化定义为这样的一个过程，它包含一系列步骤，依次是转换、清洗以及将文本 数据标准化成可供 NLP、分析系统和应用程序使用的格式。通常，文本切分本身也是文本规范化的一部分。除了文本切分以外，还有各种其他技术，包括文本清洗、大小写转换、词语校正、停用词删除、词干提取和词形还原。文本规范化也常常称为文本清洗或转换。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载工具库\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from pprint import pprint\n",
    "\n",
    "# 文本\n",
    "corpus = [\"The brown fox wasn't that quick and he couldn't win the race\",\"Hey that's a great deal! I just bought a phone for $199\",\n",
    "       \"@@You'11 (learn) a **lot** in the book. Python is an amazing language!@@\"]"
   ]
  },
  {
   "source": [
    "## 2.1 文本清洗"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "通常，我们要使用或分析的文本数据都包含大量无关和不必要的标识和字符，在进行其他操作(如切分和其他规范化操作)之前，应该先删除它们。这包括从如 HTML 之类的数据源中提取有意义的文本，数据源中可能包含不必要的 HTML 标记，甚至是来自 XML 和 JSON feed 的数据。解析并清洗这些数据的方法很多，以删除不必要的标签。你可以使用定义的逻辑，包括正则表达式、xpath 和 lxml 库来解析 XML 数据。从 JSON 获取数据较为容易，因为它具有明确的键值注释。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2.2 文本切分"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "通常，在删除数据中多余字符和符号操作的前后，进行文本切分操作。文本切分和删除 多余字符的顺序取决于你要解决的问题和你正在处理的数据。上一节已经介绍了各种切分 技术，这里会定义一个通用的切分函数，并在前面提到的语料库中运行它。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[['The',\n   'brown',\n   'fox',\n   'was',\n   \"n't\",\n   'that',\n   'quick',\n   'and',\n   'he',\n   'could',\n   \"n't\",\n   'win',\n   'the',\n   'race']],\n [['Hey', 'that', \"'s\", 'a', 'great', 'deal', '!'],\n  ['I', 'just', 'bought', 'a', 'phone', 'for', '$', '199']],\n [['@',\n   '@',\n   \"You'11\",\n   '(',\n   'learn',\n   ')',\n   'a',\n   '**lot**',\n   'in',\n   'the',\n   'book',\n   '.'],\n  ['Python', 'is', 'an', 'amazing', 'language', '!'],\n  ['@', '@']]]\n"
     ]
    }
   ],
   "source": [
    "# 定义文本切分函数\n",
    "def tokenize_text(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    word_tokens = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "# 文本切分\n",
    "token_list = [tokenize_text(text) for text in corpus]\n",
    "pprint(token_list)"
   ]
  },
  {
   "source": [
    "## 2.3 删除特殊字符"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "文本规范化中的一个重要任务是删除多余和特殊的字符，诸如特殊符号或标点符号。这个步骤通常在切分操作前后进行。这样做的主要原因是：当我们分析文本并提取基于 NLP 和机器学习的特征或信息时，标点符号或特殊字符往往没有多大的意义。我们将在切分前后 删除这两类特殊的字符。以下代码段显示了如何在切分之后删除特殊字符："
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[['The', 'brown', 'fox', 'was', 'nt', 'that', 'quick', 'and', 'he', 'could', 'nt', 'win', 'the', 'race']], [['Hey', 'that', 's', 'a', 'great', 'deal'], ['I', 'just', 'bought', 'a', 'phone', 'for', '199']], [['You11', 'learn', 'a', 'lot', 'in', 'the', 'book'], ['Python', 'is', 'an', 'amazing', 'language']]]\n"
     ]
    }
   ],
   "source": [
    "# 定义删除特殊字符的函数\n",
    "def remove_characters_after_tokenization(tokens):\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    '''\n",
    "    re.compile():将内部的字符转化为正则表达式能够识别的符号\n",
    "    re.escape():将可能被解释为正则运算符的字符进行转义\n",
    "    string.punctuation:包含所有的标点符号\n",
    "    re.sub():将符合正则表达式的符号转化为想要的符号\n",
    "    '''\n",
    "    filtered_tokens = filter(None,[pattern.sub('',token) for token in tokens])\n",
    "    return list(filtered_tokens)\n",
    "\n",
    "# 删除特殊字符\n",
    "filtered_list1 = [list(filter(None,[remove_characters_after_tokenization(tokens) for tokens in sentence_tokens])) for sentence_tokens in token_list]\n",
    "print(filtered_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}