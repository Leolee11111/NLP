{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "d9d363e889bac6e082d872d6c6a952a67371d63acf869877e6b992428fa21dbf"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 1.文本切分"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "一段文本或一个文本文件具有几个组成部分，包括可以进一步细分为从句、短语和单词的语句。最流行的文本切分技术包括句子切分和词语切分，用于将文本语料库分解成句子，并将每个句子分解成单词。因此，文本切分可以定义为将文本数据分 解或拆分为具有更小且有意义的成分（即标识）的过程。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1.1 句子切分"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "句子切分(sentence tokenization)是将文本语料库分解为句子的过程，这些句子是组成语料库的第一季切分结果，这个过程也叫句子分割。我们尝试将文本分割成有意义的句子。执行句子切分有多种技术，基本技术包括在句子之间寻找特定的分隔符，例如句号(.)、换行符(\\n)或者分号(;)。我们将使用NLTK框架进行切分，该框架提供用于执行句子切分的各种接口：                                \n",
    "   - sent_tokenize\n",
    "   - PunktSentence Tokenization\n",
    "   - Regexp Tokenization\n",
    "   - 预训练的句子切分模型                     \n",
    "\n",
    "我们使用NLTK中古腾堡(Gutenberg)语料库："
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from pprint import pprint"
   ]
  },
  {
   "source": [
    "注：print()和pprint()都是python的打印模块，功能基本一样，唯一的区别就是pprint()模块打印出来的数据结构更加完整，每行为一个数据结构，更加方便阅读打印输出结果。特别是对于特别长的数据打印，print()输出结果都在一行，不方便查看，而pprint()采用分行打印输出，所以对于数据结构比较复杂、数据长度较长的数据，适合采用pprint()打印方式。当然，一般情况多数采用print()。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n[nltk_data]     C:\\Users\\Leo\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# 下载古登堡数据集\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载文本集\n",
    "alice = gutenberg.raw(fileids=\"carroll-alice.txt\")\n",
    "sample_text = 'we will discuss briefly about the basic syntax, structure and design philosophies. There is a defined hierarchical syntax for Python code which you should remember when writing code! Python is a really powerful programming language!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "144395\n[Alice's Adventures in Wonderland by Lewis Carroll 1865]\n\nCHAPTER I. Down the Rabbit-Hole\n\nAlice was\n"
     ]
    }
   ],
   "source": [
    "# 查看Alice in Wonderland 语料库的长度\n",
    "print(len(alice))\n",
    "# 查看Alice in Wonderland 语料库前100个字符\n",
    "print(alice[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total sentences in sample_text: 3\nsample text sentences :-\n['we will discuss briefly about the basic syntax, structure and design philosophies.', 'There is a defined hierarchical syntax for Python code which you should remember when writing code!', 'Python is a really powerful programming language!']\n\nTotal sentences in alice: 1625\nFirst 5 sentences in alice :-\n[\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I.\", \"Down the Rabbit-Hole\\n\\nAlice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to do: once or twice she had peeped into the\\nbook her sister was reading, but it had no pictures or conversations in\\nit, 'and what is the use of a book,' thought Alice 'without pictures or\\nconversation?'\", 'So she was considering in her own mind (as well as she could, for the\\nhot day made her feel very sleepy and stupid), whether the pleasure\\nof making a daisy-chain would be worth the trouble of getting up and\\npicking the daisies, when suddenly a White Rabbit with pink eyes ran\\nclose by her.', \"There was nothing so VERY remarkable in that; nor did Alice think it so\\nVERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\", 'Oh dear!']\n"
     ]
    }
   ],
   "source": [
    "# nltk.sent_tokenize是nltk默认的句子切分函数\n",
    "default_st = nltk.sent_tokenize\n",
    "alice_sentences = default_st(text=alice)\n",
    "sample_sentences = default_st(text=sample_text)\n",
    "print('Total sentences in sample_text:',len(sample_sentences))\n",
    "print('sample text sentences :-')\n",
    "print(sample_sentences)\n",
    "print('\\nTotal sentences in alice:',len(alice_sentences))\n",
    "print('First 5 sentences in alice :-')\n",
    "print(alice_sentences[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[nltk_data] Downloading package europarl_raw to\n[nltk_data]     C:\\Users\\Leo\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package europarl_raw is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# 下面来看看德国文档(下载德语文本语料库)\n",
    "nltk.download('europarl_raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "157171\n \nWiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sit\nTrue\n \nWiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen , wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe , daß Sie schöne Ferien hatten .\nWie Sie feststellen konnten , ist der gefürchtete \" Millenium-Bug \" nicht eingetreten .\nDoch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden .\nIm Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser Sitzungsperiode in den nächsten Tagen .\nHeute möchte ich Sie bitten - das ist auch der Wunsch einiger Kolleginnen und Kollegen - , allen Opfern der Stürme , insbesondere in den verschiedenen Ländern der Europäischen Union , in einer Schweigeminute zu gedenken .\n"
     ]
    }
   ],
   "source": [
    "# 下面对德语文本进行句子切分\n",
    "from nltk.corpus import europarl_raw\n",
    "german_text = europarl_raw.german.raw(fileids='ep-00-01-17.de')\n",
    "# 文本的总句子数\n",
    "print(len(german_text))\n",
    "# 德语文本前100个字符\n",
    "print(german_text[0:100])\n",
    "# 使用默认的sent_tokenize 切分器\n",
    "german_sentences_def = default_st(text=german_text,language='german')\n",
    "# 使用从nltk源加载的预训练的德语切分器\n",
    "german_tokenizer = nltk.data.load(resource_url='tokenizers/punkt/german.pickle')\n",
    "german_sentences = german_tokenizer.tokenize(german_text)\n",
    "# 判断默认方式下的切分与预训练的德语切分器的结果是否一致\n",
    "print(german_sentences_def == german_sentences)\n",
    "# 输出预训练切分句子的前五句\n",
    "for sent in german_sentences[0:5]:\n",
    "    print(sent)"
   ]
  },
  {
   "source": [
    "我们得出的结论是：默认切分器和预训练切分器的结果完全一致"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "最后再介绍一种基于正则表达式的句子切分方式，使用RegexpTokenizer类的示例切分为句子："
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['we will discuss briefly about the basic syntax, structure and design philosophies.', 'There is a defined hierarchical syntax for Python code which you should remember when writing code!', 'Python is a really powerful programming language!']\n"
     ]
    }
   ],
   "source": [
    "SENTENCE_TOKENS_PATTERN = r'(?<!\\w\\.\\w)(?<![A-Z][a-z]\\.)(?<![A-Z]\\.)(?<=\\.|\\?|\\!)\\s'\n",
    "regex_st = nltk.tokenize.RegexpTokenizer(pattern=SENTENCE_TOKENS_PATTERN,gaps=True)\n",
    "sample_sentences = regex_st.tokenize(sample_text)\n",
    "print(sample_sentences)"
   ]
  },
  {
   "source": [
    "## 1.2 词语切分"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "词语切分（word tokenization）是将句子分解或分割成其组成单词的过程。句子是单词的集合，通过词语切分，在本质上，将一个句子分割成单词列表，该单词列表又可以重建句子。词语切分在很多过程中都是非常重要的，特别是在文本清洗和规范化时，诸如词干提取 和词形还原这类基于词干、标识信息的操作会在每个单词上实施。与句子切分类似，nltk为词语切分提供了各种有用的接口：                           \n",
    "   - word_tokenize\n",
    "   - TreebankWordTokenizer\n",
    "   - RegexpTokenizer\n",
    "   - 从RegexpTokenizer继承的切分器"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['The', 'brown', 'fox', 'was', \"n't\", 'that', 'quick', 'and', 'he', 'could', \"n't\", 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "# 我们使用例句作为默认切分器的输入，并切分成词语\n",
    "sentence = \"The brown fox wasn't that quick and he couldn't win the race\"\n",
    "default_wt = nltk.word_tokenize\n",
    "words = default_wt(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['The', 'brown', 'fox', 'was', \"n't\", 'that', 'quick', 'and', 'he', 'could', \"n't\", 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "# 使用TreebankWordTokenizer类切分句子称为单词\n",
    "treebank_wt = nltk.TreebankWordTokenizer()\n",
    "words = treebank_wt.tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "source": [
    "# 2.文本规范化"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "文本规范化定义为这样的一个过程，它包含一系列步骤，依次是转换、清洗以及将文本 数据标准化成可供 NLP、分析系统和应用程序使用的格式。通常，文本切分本身也是文本规范化的一部分。除了文本切分以外，还有各种其他技术，包括文本清洗、大小写转换、词语校正、停用词删除、词干提取和词形还原。文本规范化也常常称为文本清洗或转换。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载工具库\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from pprint import pprint\n",
    "\n",
    "# 文本\n",
    "corpus = [\"The brown fox wasn't that quick and he couldn't win the race\",\"Hey that's a great deal! I just bought a phone for $199\",\n",
    "       \"@@You'll (learn) a **lot** in the book. Python is an amazing language!@@\"]"
   ]
  },
  {
   "source": [
    "## 2.1 文本清洗"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "通常，我们要使用或分析的文本数据都包含大量无关和不必要的标识和字符，在进行其他操作(如切分和其他规范化操作)之前，应该先删除它们。这包括从如 HTML 之类的数据源中提取有意义的文本，数据源中可能包含不必要的 HTML 标记，甚至是来自 XML 和 JSON feed 的数据。解析并清洗这些数据的方法很多，以删除不必要的标签。你可以使用定义的逻辑，包括正则表达式、xpath 和 lxml 库来解析 XML 数据。从 JSON 获取数据较为容易，因为它具有明确的键值注释。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2.2 文本切分"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "通常，在删除数据中多余字符和符号操作的前后，进行文本切分操作。文本切分和删除 多余字符的顺序取决于你要解决的问题和你正在处理的数据。上一节已经介绍了各种切分 技术，这里会定义一个通用的切分函数，并在前面提到的语料库中运行它。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[['The',\n   'brown',\n   'fox',\n   'was',\n   \"n't\",\n   'that',\n   'quick',\n   'and',\n   'he',\n   'could',\n   \"n't\",\n   'win',\n   'the',\n   'race']],\n [['Hey', 'that', \"'s\", 'a', 'great', 'deal', '!'],\n  ['I', 'just', 'bought', 'a', 'phone', 'for', '$', '199']],\n [['@',\n   '@',\n   'You',\n   \"'ll\",\n   '(',\n   'learn',\n   ')',\n   'a',\n   '**lot**',\n   'in',\n   'the',\n   'book',\n   '.'],\n  ['Python', 'is', 'an', 'amazing', 'language', '!'],\n  ['@', '@']]]\n"
     ]
    }
   ],
   "source": [
    "# 定义文本切分函数\n",
    "def tokenize_text(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    word_tokens = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "# 文本切分\n",
    "token_list = [tokenize_text(text) for text in corpus]\n",
    "pprint(token_list)"
   ]
  },
  {
   "source": [
    "## 2.3 删除特殊字符"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "文本规范化中的一个重要任务是删除多余和特殊的字符，诸如特殊符号或标点符号。这个步骤通常在切分操作前后进行。这样做的主要原因是：当我们分析文本并提取基于 NLP 和机器学习的特征或信息时，标点符号或特殊字符往往没有多大的意义。我们将在切分前后 删除这两类特殊的字符。以下代码段显示了如何在切分之后删除特殊字符："
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[['The', 'brown', 'fox', 'was', 'nt', 'that', 'quick', 'and', 'he', 'could', 'nt', 'win', 'the', 'race']], [['Hey', 'that', 's', 'a', 'great', 'deal'], ['I', 'just', 'bought', 'a', 'phone', 'for', '199']], [['You', 'll', 'learn', 'a', 'lot', 'in', 'the', 'book'], ['Python', 'is', 'an', 'amazing', 'language']]]\n"
     ]
    }
   ],
   "source": [
    "# 定义删除所有特殊字符的函数\n",
    "def remove_characters_after_tokenization(tokens):\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    '''\n",
    "    re.compile():将内部的字符转化为正则表达式能够识别的符号\n",
    "    re.escape():将可能被解释为正则运算符的字符进行转义\n",
    "    string.punctuation:包含所有的标点符号\n",
    "    re.sub():将符合正则表达式的符号转化为想要的符号\n",
    "    '''\n",
    "    filtered_tokens = filter(None,[pattern.sub('',token) for token in tokens])\n",
    "    return list(filtered_tokens)\n",
    "\n",
    "# 删除特殊字符\n",
    "filtered_list1 = [list(filter(None,[remove_characters_after_tokenization(tokens) for tokens in sentence_tokens])) for sentence_tokens in token_list]\n",
    "print(filtered_list1)"
   ]
  },
  {
   "source": [
    "# 删除除了\"'\"和句号的特殊字符的函数\n",
    "def remove_characters_before_tokenization(sentence,keep_apostrophes = False):\n",
    "    sentence = sentence.strip()\n",
    "    if keep_apostrophes:\n",
    "        PATTERN = r'[?|$|&|*|%|@|(|)~]'   # 定义一些需要删除的字符\n",
    "        filtered_sentence = re.sub(PATTERN,r'',sentence)\n",
    "    else:\n",
    "        PATTERN = r'[^a-zA-Z0-9 ]'  # ^...用于匹配^后不存在的字符，因此只留下非字母和数字的字符\n",
    "        filtered_sentence = re.sub(PATTERN,r'',sentence)\n",
    "\n",
    "    return filtered_sentence\n",
    "\n",
    "filtered_list2 = [remove_characters_before_tokenization(sentence) for sentence in corpus]\n",
    "print(filtered_list2)\n",
    "cleaned_corpus = [remove_characters_before_tokenization(sentence, keep_apostrophes=True) for sentence in corpus]\n",
    "print(cleaned_corpus)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['The brown fox wasnt that quick and he couldnt win the race', 'Hey thats a great deal I just bought a phone for 199', 'Youll learn a lot in the book Python is an amazing language']\n[\"The brown fox wasn't that quick and he couldn't win the race\", \"Hey that's a great deal! I just bought a phone for 199\", \"You'll learn a lot in the book. Python is an amazing language!\"]\n"
     ]
    }
   ]
  },
  {
   "source": [
    "## 2.4 扩展缩写词"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "缩写词（contraction）是词或音节的缩短形式。它们既在书面形式中存在，也在口语中存在。现有单司的缩短版本可以通过删除特定的字母和音节获得。在英语的缩写形式中，缩写词 通常是从单词中删除一些元音来创建的。举例来说“is not\" 缩写成“isn' t\"，“will not”缩 写成“won' t”，你应该已经注意到了，缩写词中撇号用来表示缩写，而一些元音和其他字母则被删除了。通常，在正式书写时会避免使用缩写词，但在非正式情况下，它们被广泛使用。\n",
    "英语中存在各种形式的缩写词，这些形式的缩写词与助动词的类型相关，不同的助动词给出了 常规缩写词、否定缩写词和其他特殊的口语缩写词（其中一些可能并不涉及助动词）。 缩写词确实为 NLP 和文本分析制造了一个难题，首先因为在该单词中有一个特殊的撒 号字符。此外，我们有两个甚至更多的单词由缩写词表示，当尝试执行词语切分或者词语标 准化时，这就会引发一连串复杂的问题。因此，在处理文本时，需要一些确切的步骤来处理 缩写词。理想情况下，你可以对缩写词和对应的扩展词语进行适当的映射，然后使用映射关系扩展文本中的所有缩写词。其中有一个缩写词及其扩展形式的词汇表CONTRACTION_MAP.py   "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['The brown fox was not that quick and he could not win the race', 'Hey that is a great deal! I just bought a phone for 199', 'You will learn a lot in the book. Python is an amazing language!']\n"
     ]
    }
   ],
   "source": [
    "from contractions import CONTRACTION_MAP\n",
    "\n",
    "def expend_contractions(sentence,contraction_mapping):\n",
    "    contractions_pattern = re.compile('({})'.format(\"|\".join(contraction_mapping.keys())),flags=re.IGNORECASE|re.DOTALL)\n",
    "    '''\n",
    "    |：代表or，用于匹配前一个或者后一个正则表达式\n",
    "    flags参数：匹配的模式\n",
    "    re.IGNORECASE：忽略大小写\n",
    "    re.DOTALL：默认情况下，正则表达式中的dot（.），表示所有除了换行的字符，加上re.DOTALL参数后，就是真正的所有字符了，包括换行符（\\n）\n",
    "    '''\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0) \n",
    "        \"\"\"\n",
    "        group(0) :group() 同group（0）就是匹配正则表达式整体结果,group(1) 列出第一个括号匹配部分，group(2) 列出第二个括号匹配部分，group(3) 列出第三个括号匹配部分。\n",
    "        get(key):返回指定键的值，如果键不在字典中返回默认值 None 或者设置的默认值。\n",
    "\n",
    "        \"\"\"\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match) if contraction_mapping.get(match) else contraction_mapping.get(match.lower())\n",
    "        expanded_contraction = first_char + expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "    expanded_sentence = contractions_pattern.sub(expand_match,sentence)\n",
    "    return expanded_sentence\n",
    "\n",
    "\n",
    "expanded_corpus = [expend_contractions(sentence,CONTRACTION_MAP) for sentence in cleaned_corpus]\n",
    "print(expanded_corpus)"
   ]
  },
  {
   "source": [
    "## 2.5 大小写转换"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "the brown fox wasn't that quick and he couldn't win the race\nTHE BROWN FOX WASN'T THAT QUICK AND HE COULDN'T WIN THE RACE\n"
     ]
    }
   ],
   "source": [
    "# 将所有的英文转化为小写\n",
    "print(corpus[0].lower())\n",
    "# 将所有的英文转换为大写\n",
    "print(corpus[0].upper())"
   ]
  },
  {
   "source": [
    "## 2.6 删除停用词"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "停用词（stopword，有时也拼写成 stop word）是指没有或只有极小意义的词语。通常在处理过程中将它们从文本中删除，以保留具有最大意义及语境的词语。如果你基于单个标识聚合语料库，然后检查词语频率，就会发现停用词的出现频率是最高的。类似\"a\" \"the\" \"me\"和\"and so on\"这样的单词就是停用词。目前还没有普遍或已穷尽的停用词列表。每 个领域或语言可能都有一系列独有的停用词。以下代码段展示了一种过滤和删除英语停用词的方法："
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\Leo\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "# 下载英文停用词\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[['The', 'brown', 'fox', 'quick', 'could', 'win', 'race']], [['Hey', 'great', 'deal', '!'], ['I', 'bought', 'phone', '199']], [['You', 'learn', 'lot', 'book', '.'], ['Python', 'amazing', 'language', '!']]]\n"
     ]
    }
   ],
   "source": [
    "# 定义删除停用词函数\n",
    "def remove_stopwords(tokens):\n",
    "    # 加载英文停用词列表\n",
    "    stopwprd_list = nltk.corpus.stopwords.words('english')\n",
    "    filtered_tokens = [token for token in tokens if token not in stopwprd_list]\n",
    "    return filtered_tokens\n",
    "\n",
    "\n",
    "# 使用上一节获得的expanded_corpus，然后删除停用词\n",
    "expanded_corpus_tokens = [tokenize_text(text) for text in expanded_corpus]\n",
    "filtered_list_3 = [[remove_stopwords(tokens) for tokens in sentence_tokens] for sentence_tokens in expanded_corpus_tokens]\n",
    "print(filtered_list_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# 打印nltk.corpus.stopwords.words('english')\n",
    "print(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "source": [
    "## 2.7 词语校正"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 2.7.1 校正重复的词语"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Leo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "# 下载wordnet语料库\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['My', 'schooool', 'is', 'realllllyyy', 'amaaazingggg']\n",
      "['My', 'school', 'is', 'really', 'amazing']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "# 定义删除重复字符同时又保留正确单词的函数\n",
    "def remove_repeated_characters(tokens):\n",
    "    '''\n",
    "    \\2代表跟第二个括号内的字符一样\n",
    "    r'(\\w*)(\\w)\\2(\\w*)':代表重复第二个括号内容\n",
    "    repeat_pattern.sub(match_substitution,old_word)代表由r'(\\w*)(\\w)\\2(\\w*)'替换成r'(\\w*)(\\w)(\\w*)'\n",
    "    wordnet.synsets(old_word)：返回oldword的所有形式\n",
    "    '''\n",
    "    repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    match_substitution = r'\\1\\2\\3'\n",
    "    def replace(old_word):\n",
    "        if wordnet.synsets(old_word):\n",
    "            return old_word\n",
    "        new_word = repeat_pattern.sub(match_substitution,old_word)\n",
    "        return replace(new_word) if new_word != old_word else new_word\n",
    "\n",
    "    correct_tokens = [replace(word) for word in tokens]\n",
    "    return correct_tokens\n",
    "\n",
    "sample_sentence = \"My schooool is realllllyyy amaaazingggg\"\n",
    "sample_sentence_tokens = tokenize_text(sample_sentence)[0]\n",
    "print(sample_sentence_tokens)\n",
    "print(remove_repeated_characters(sample_sentence_tokens))\n"
   ]
  },
  {
   "source": [
    "### 2.7.2 校正拼写错误"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "我们面临的另一个问题是由人为错误导致的拼写错误，甚至是由于自动更正文本等功能导致的机器拼写错误。有多种处理拼写错误的方法，其最终目标都是获得拼写正确的文本标识。本节将介绍最为著名的算法之一，它由谷歌研究主管 Peter Norvig 开发。你可以在http://norvig.com/spell-correct.html 上找到完整详细的算法说明。                                     \n",
    "我们的主要目标是，给出一个单词，找到这个单词最有可能的正确形式。我们遵循的方法是生成一系列类似输人词的候选词，并从该集合中选择最有可能的单词作为正确的单词 我们使用标准英文单词语料库，根据语料库中单词的频率，从距离输入单词最近的最后一组 候选词中识别出正确的单词。这个距离（即一个单词与输入单词的测量距离）也称为编辑国家语料库中的最常用单词列表。你可以在本章的代码资源中找到这个命名为 big. txt 的文件，或者从 http://norvig.com/big.txt 下载它。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('the', 80030), ('of', 40025), ('and', 38313), ('to', 28766), ('in', 22050), ('a', 21155), ('that', 12512), ('he', 12401), ('was', 11410), ('it', 10681)]\n"
     ]
    }
   ],
   "source": [
    "# 拥有自己的词汇\n",
    "import re \n",
    "import collections\n",
    "\n",
    "def tokens(text):\n",
    "    \"\"\"\n",
    "    获取语料库中所有的单词\n",
    "    \"\"\"\n",
    "    return re.findall('[a-z]+',text.lower())\n",
    "\n",
    "WORDS = tokens(open('big.txt').read())\n",
    "WORD_COUNTS = collections.Counter(WORDS)\n",
    "\n",
    "print(WORD_COUNTS.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拥有了自己的词汇之后，就可以定义三个函数，计算出与输入单词的编辑距离为0,1,2的单词组。这些编辑距离由插入、删除、添加和调换位置等操作产生。\n",
    "def edits0(word): \n",
    "    \"\"\"\n",
    "    Return all strings that are zero edits away \n",
    "    from the input word (i.e., the word itself).\n",
    "    \"\"\"\n",
    "    return {word}\n",
    "\n",
    "\n",
    "\n",
    "def edits1(word):\n",
    "    \"\"\"\n",
    "    Return all strings that are one edit away \n",
    "    from the input word.\n",
    "    \"\"\"\n",
    "    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    def splits(word):\n",
    "        \"\"\"\n",
    "        Return a list of all possible (first, rest) pairs \n",
    "        that the input word is made of.\n",
    "        \"\"\"\n",
    "        return [(word[:i], word[i:]) \n",
    "                for i in range(len(word)+1)]\n",
    "                \n",
    "    pairs      = splits(word)\n",
    "    deletes    = [a+b[1:]           for (a, b) in pairs if b]\n",
    "    transposes = [a+b[1]+b[0]+b[2:] for (a, b) in pairs if len(b) > 1]\n",
    "    replaces   = [a+c+b[1:]         for (a, b) in pairs for c in alphabet if b]\n",
    "    inserts    = [a+c+b             for (a, b) in pairs for c in alphabet]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "\n",
    "def edits2(word):\n",
    "    \"\"\"Return all strings that are two edits away \n",
    "    from the input word.\n",
    "    \"\"\"\n",
    "    return {e2 for e1 in edits1(word) for e2 in edits1(e1)}"
   ]
  },
  {
   "source": [
    "# 我们还可以定义一个 known( ) 函数，该函数根据单词是否存在于词汇词典 WORD_COUNTS 中，从 edit 函数得出的候选词组中返回一个单词子集。这使我们可以从候选词组中获得一 个有效单词列表：\n",
    "def known(words):\n",
    "    \"\"\"\n",
    "    Return the subset of words that are actually \n",
    "    in our WORD_COUNTS dictionary.\n",
    "    \"\"\"\n",
    "    return {w for w in words if w in WORD_COUNTS}"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'fianlly'}"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "# input word\n",
    "word = 'fianlly'\n",
    "\n",
    "# zero edit distance from input word\n",
    "edits0(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "# returns null set since it is not a valid word\n",
    "known(edits0(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'afianlly',\n",
       " 'aianlly',\n",
       " 'bfianlly',\n",
       " 'bianlly',\n",
       " 'cfianlly',\n",
       " 'cianlly',\n",
       " 'dfianlly',\n",
       " 'dianlly',\n",
       " 'efianlly',\n",
       " 'eianlly',\n",
       " 'faanlly',\n",
       " 'faianlly',\n",
       " 'fainlly',\n",
       " 'fanlly',\n",
       " 'fbanlly',\n",
       " 'fbianlly',\n",
       " 'fcanlly',\n",
       " 'fcianlly',\n",
       " 'fdanlly',\n",
       " 'fdianlly',\n",
       " 'feanlly',\n",
       " 'feianlly',\n",
       " 'ffanlly',\n",
       " 'ffianlly',\n",
       " 'fganlly',\n",
       " 'fgianlly',\n",
       " 'fhanlly',\n",
       " 'fhianlly',\n",
       " 'fiaally',\n",
       " 'fiaanlly',\n",
       " 'fiablly',\n",
       " 'fiabnlly',\n",
       " 'fiaclly',\n",
       " 'fiacnlly',\n",
       " 'fiadlly',\n",
       " 'fiadnlly',\n",
       " 'fiaelly',\n",
       " 'fiaenlly',\n",
       " 'fiaflly',\n",
       " 'fiafnlly',\n",
       " 'fiaglly',\n",
       " 'fiagnlly',\n",
       " 'fiahlly',\n",
       " 'fiahnlly',\n",
       " 'fiailly',\n",
       " 'fiainlly',\n",
       " 'fiajlly',\n",
       " 'fiajnlly',\n",
       " 'fiaklly',\n",
       " 'fiaknlly',\n",
       " 'fiallly',\n",
       " 'fially',\n",
       " 'fialnlly',\n",
       " 'fialnly',\n",
       " 'fiamlly',\n",
       " 'fiamnlly',\n",
       " 'fianally',\n",
       " 'fianaly',\n",
       " 'fianblly',\n",
       " 'fianbly',\n",
       " 'fianclly',\n",
       " 'fiancly',\n",
       " 'fiandlly',\n",
       " 'fiandly',\n",
       " 'fianelly',\n",
       " 'fianely',\n",
       " 'fianflly',\n",
       " 'fianfly',\n",
       " 'fianglly',\n",
       " 'fiangly',\n",
       " 'fianhlly',\n",
       " 'fianhly',\n",
       " 'fianilly',\n",
       " 'fianily',\n",
       " 'fianjlly',\n",
       " 'fianjly',\n",
       " 'fianklly',\n",
       " 'fiankly',\n",
       " 'fianlaly',\n",
       " 'fianlay',\n",
       " 'fianlbly',\n",
       " 'fianlby',\n",
       " 'fianlcly',\n",
       " 'fianlcy',\n",
       " 'fianldly',\n",
       " 'fianldy',\n",
       " 'fianlely',\n",
       " 'fianley',\n",
       " 'fianlfly',\n",
       " 'fianlfy',\n",
       " 'fianlgly',\n",
       " 'fianlgy',\n",
       " 'fianlhly',\n",
       " 'fianlhy',\n",
       " 'fianlily',\n",
       " 'fianliy',\n",
       " 'fianljly',\n",
       " 'fianljy',\n",
       " 'fianlkly',\n",
       " 'fianlky',\n",
       " 'fianll',\n",
       " 'fianlla',\n",
       " 'fianllay',\n",
       " 'fianllb',\n",
       " 'fianllby',\n",
       " 'fianllc',\n",
       " 'fianllcy',\n",
       " 'fianlld',\n",
       " 'fianlldy',\n",
       " 'fianlle',\n",
       " 'fianlley',\n",
       " 'fianllf',\n",
       " 'fianllfy',\n",
       " 'fianllg',\n",
       " 'fianllgy',\n",
       " 'fianllh',\n",
       " 'fianllhy',\n",
       " 'fianlli',\n",
       " 'fianlliy',\n",
       " 'fianllj',\n",
       " 'fianlljy',\n",
       " 'fianllk',\n",
       " 'fianllky',\n",
       " 'fianlll',\n",
       " 'fianllly',\n",
       " 'fianllm',\n",
       " 'fianllmy',\n",
       " 'fianlln',\n",
       " 'fianllny',\n",
       " 'fianllo',\n",
       " 'fianlloy',\n",
       " 'fianllp',\n",
       " 'fianllpy',\n",
       " 'fianllq',\n",
       " 'fianllqy',\n",
       " 'fianllr',\n",
       " 'fianllry',\n",
       " 'fianlls',\n",
       " 'fianllsy',\n",
       " 'fianllt',\n",
       " 'fianllty',\n",
       " 'fianllu',\n",
       " 'fianlluy',\n",
       " 'fianllv',\n",
       " 'fianllvy',\n",
       " 'fianllw',\n",
       " 'fianllwy',\n",
       " 'fianllx',\n",
       " 'fianllxy',\n",
       " 'fianlly',\n",
       " 'fianllya',\n",
       " 'fianllyb',\n",
       " 'fianllyc',\n",
       " 'fianllyd',\n",
       " 'fianllye',\n",
       " 'fianllyf',\n",
       " 'fianllyg',\n",
       " 'fianllyh',\n",
       " 'fianllyi',\n",
       " 'fianllyj',\n",
       " 'fianllyk',\n",
       " 'fianllyl',\n",
       " 'fianllym',\n",
       " 'fianllyn',\n",
       " 'fianllyo',\n",
       " 'fianllyp',\n",
       " 'fianllyq',\n",
       " 'fianllyr',\n",
       " 'fianllys',\n",
       " 'fianllyt',\n",
       " 'fianllyu',\n",
       " 'fianllyv',\n",
       " 'fianllyw',\n",
       " 'fianllyx',\n",
       " 'fianllyy',\n",
       " 'fianllyz',\n",
       " 'fianllz',\n",
       " 'fianllzy',\n",
       " 'fianlmly',\n",
       " 'fianlmy',\n",
       " 'fianlnly',\n",
       " 'fianlny',\n",
       " 'fianloly',\n",
       " 'fianloy',\n",
       " 'fianlply',\n",
       " 'fianlpy',\n",
       " 'fianlqly',\n",
       " 'fianlqy',\n",
       " 'fianlrly',\n",
       " 'fianlry',\n",
       " 'fianlsly',\n",
       " 'fianlsy',\n",
       " 'fianltly',\n",
       " 'fianlty',\n",
       " 'fianluly',\n",
       " 'fianluy',\n",
       " 'fianlvly',\n",
       " 'fianlvy',\n",
       " 'fianlwly',\n",
       " 'fianlwy',\n",
       " 'fianlxly',\n",
       " 'fianlxy',\n",
       " 'fianly',\n",
       " 'fianlyl',\n",
       " 'fianlyly',\n",
       " 'fianlyy',\n",
       " 'fianlzly',\n",
       " 'fianlzy',\n",
       " 'fianmlly',\n",
       " 'fianmly',\n",
       " 'fiannlly',\n",
       " 'fiannly',\n",
       " 'fianolly',\n",
       " 'fianoly',\n",
       " 'fianplly',\n",
       " 'fianply',\n",
       " 'fianqlly',\n",
       " 'fianqly',\n",
       " 'fianrlly',\n",
       " 'fianrly',\n",
       " 'fianslly',\n",
       " 'fiansly',\n",
       " 'fiantlly',\n",
       " 'fiantly',\n",
       " 'fianully',\n",
       " 'fianuly',\n",
       " 'fianvlly',\n",
       " 'fianvly',\n",
       " 'fianwlly',\n",
       " 'fianwly',\n",
       " 'fianxlly',\n",
       " 'fianxly',\n",
       " 'fianylly',\n",
       " 'fianyly',\n",
       " 'fianzlly',\n",
       " 'fianzly',\n",
       " 'fiaolly',\n",
       " 'fiaonlly',\n",
       " 'fiaplly',\n",
       " 'fiapnlly',\n",
       " 'fiaqlly',\n",
       " 'fiaqnlly',\n",
       " 'fiarlly',\n",
       " 'fiarnlly',\n",
       " 'fiaslly',\n",
       " 'fiasnlly',\n",
       " 'fiatlly',\n",
       " 'fiatnlly',\n",
       " 'fiaully',\n",
       " 'fiaunlly',\n",
       " 'fiavlly',\n",
       " 'fiavnlly',\n",
       " 'fiawlly',\n",
       " 'fiawnlly',\n",
       " 'fiaxlly',\n",
       " 'fiaxnlly',\n",
       " 'fiaylly',\n",
       " 'fiaynlly',\n",
       " 'fiazlly',\n",
       " 'fiaznlly',\n",
       " 'fibanlly',\n",
       " 'fibnlly',\n",
       " 'ficanlly',\n",
       " 'ficnlly',\n",
       " 'fidanlly',\n",
       " 'fidnlly',\n",
       " 'fieanlly',\n",
       " 'fienlly',\n",
       " 'fifanlly',\n",
       " 'fifnlly',\n",
       " 'figanlly',\n",
       " 'fignlly',\n",
       " 'fihanlly',\n",
       " 'fihnlly',\n",
       " 'fiianlly',\n",
       " 'fiinlly',\n",
       " 'fijanlly',\n",
       " 'fijnlly',\n",
       " 'fikanlly',\n",
       " 'fiknlly',\n",
       " 'filanlly',\n",
       " 'filnlly',\n",
       " 'fimanlly',\n",
       " 'fimnlly',\n",
       " 'finally',\n",
       " 'finanlly',\n",
       " 'finlly',\n",
       " 'finnlly',\n",
       " 'fioanlly',\n",
       " 'fionlly',\n",
       " 'fipanlly',\n",
       " 'fipnlly',\n",
       " 'fiqanlly',\n",
       " 'fiqnlly',\n",
       " 'firanlly',\n",
       " 'firnlly',\n",
       " 'fisanlly',\n",
       " 'fisnlly',\n",
       " 'fitanlly',\n",
       " 'fitnlly',\n",
       " 'fiuanlly',\n",
       " 'fiunlly',\n",
       " 'fivanlly',\n",
       " 'fivnlly',\n",
       " 'fiwanlly',\n",
       " 'fiwnlly',\n",
       " 'fixanlly',\n",
       " 'fixnlly',\n",
       " 'fiyanlly',\n",
       " 'fiynlly',\n",
       " 'fizanlly',\n",
       " 'fiznlly',\n",
       " 'fjanlly',\n",
       " 'fjianlly',\n",
       " 'fkanlly',\n",
       " 'fkianlly',\n",
       " 'flanlly',\n",
       " 'flianlly',\n",
       " 'fmanlly',\n",
       " 'fmianlly',\n",
       " 'fnanlly',\n",
       " 'fnianlly',\n",
       " 'foanlly',\n",
       " 'foianlly',\n",
       " 'fpanlly',\n",
       " 'fpianlly',\n",
       " 'fqanlly',\n",
       " 'fqianlly',\n",
       " 'franlly',\n",
       " 'frianlly',\n",
       " 'fsanlly',\n",
       " 'fsianlly',\n",
       " 'ftanlly',\n",
       " 'ftianlly',\n",
       " 'fuanlly',\n",
       " 'fuianlly',\n",
       " 'fvanlly',\n",
       " 'fvianlly',\n",
       " 'fwanlly',\n",
       " 'fwianlly',\n",
       " 'fxanlly',\n",
       " 'fxianlly',\n",
       " 'fyanlly',\n",
       " 'fyianlly',\n",
       " 'fzanlly',\n",
       " 'fzianlly',\n",
       " 'gfianlly',\n",
       " 'gianlly',\n",
       " 'hfianlly',\n",
       " 'hianlly',\n",
       " 'ianlly',\n",
       " 'ifanlly',\n",
       " 'ifianlly',\n",
       " 'iianlly',\n",
       " 'jfianlly',\n",
       " 'jianlly',\n",
       " 'kfianlly',\n",
       " 'kianlly',\n",
       " 'lfianlly',\n",
       " 'lianlly',\n",
       " 'mfianlly',\n",
       " 'mianlly',\n",
       " 'nfianlly',\n",
       " 'nianlly',\n",
       " 'ofianlly',\n",
       " 'oianlly',\n",
       " 'pfianlly',\n",
       " 'pianlly',\n",
       " 'qfianlly',\n",
       " 'qianlly',\n",
       " 'rfianlly',\n",
       " 'rianlly',\n",
       " 'sfianlly',\n",
       " 'sianlly',\n",
       " 'tfianlly',\n",
       " 'tianlly',\n",
       " 'ufianlly',\n",
       " 'uianlly',\n",
       " 'vfianlly',\n",
       " 'vianlly',\n",
       " 'wfianlly',\n",
       " 'wianlly',\n",
       " 'xfianlly',\n",
       " 'xianlly',\n",
       " 'yfianlly',\n",
       " 'yianlly',\n",
       " 'zfianlly',\n",
       " 'zianlly'}"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "# # one edit distance from input word\n",
    "edits1(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'finally'}"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "# get correct words from above set\n",
    "known(edits1(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'finally'}\n"
     ]
    }
   ],
   "source": [
    "candidates = (known(edits0(word)) or \n",
    "              known(edits1(word)) or \n",
    "              known(edits2(word)) or \n",
    "              [word])\n",
    "print(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'finally'"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "# 定义修正函数\n",
    "def correct(word):\n",
    "    \"\"\"\n",
    "    Get the best correct spelling for the input word\n",
    "    \"\"\"\n",
    "    # Priority is for edit distance 0, then 1, then 2\n",
    "    # else defaults to the input word itself.\n",
    "    candidates = (known(edits0(word)) or \n",
    "                  known(edits1(word)) or \n",
    "                  known(edits2(word)) or \n",
    "                  [word])\n",
    "    return max(candidates, key=WORD_COUNTS.get)\n",
    "\n",
    "correct('fianlly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'sing'"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "# python提供了纠正拼写的函数\n",
    "from textblob import Word\n",
    "\n",
    "w = Word('singg')\n",
    "w.correct()"
   ]
  },
  {
   "source": [
    "## 2.8 词干提取"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "想要理解词干提取的过程需要先理解词干（stem）的含义。谈到词素，它是任何自然语言中最小的独立单元。词素由词干和词缀（affixe）组成。词缀是指前缀、后缀等词语单元，它们附加到词干上以改变其含义或创建一个新单词。词干也经常称为单词的基本形式，我们可以通过在词干上添加词缀来创建新词，这个过程称为词形变化。相反的过程是从单词的变形形式中获得单词的基本形式，这称为词干提取。                                   \n",
    "以“JUMP”一词为例，你可以对其添加词缀形成新的单词，如“JUMPS\" “JUMPED”和“JUMPING”。在这些情况下，基本单词“JUMP\" 是词干。如果对这三种变形形式中的\n",
    "任一种进行词干提取，都将得到基本形式：                            \n",
    "\n",
    "![jupyter](./image/第三章/1.png)                 \n",
    "\n",
    "上图显示了词干在所有变形中是如何存在的，它构建了一个基础，每个词形变化都是在其上添加词缀构成的。词干提取帮助我们将词语标准化到其基础词\n",
    "干而不用考虑其词形变化，这对于许多应用程序大有脾益，如文本分类或聚类以及信息检索。搜索引擎广 泛使用这些技术来提供更好、更准确的结果，而无需考虑单词的形式。                      \n",
    "词干提取器包含在 stem 模块中，该模块继承了 nltk. stem api 模块中的 StemmerI 接口。 你甚至可以使用这个类（严格来说，它是一个接口 ) 作为你的基类来创建自己的词干提取器。目前，最受欢迎的词干提取器之一是波特词干提取器，它基于其发明人马丁・波特 (Martin Porter) 博士所开发的算法。其原始算法拥有 5 个不同的阶段，用于减少变形和提取干，其中每个阶段都有自己的一套规则。此外，还有一个 Porter2 词干提取算法，它是波 特博士在原始算法基础上提出的改进算法。以下代码段展示了波特词干提取器 :"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "print(ps.stem('jumping'),ps.stem('jumps'),ps.stem('jumped'))\n",
    "print(ps.stem('lying'))\n",
    "print(ps.stem('strange'))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 32,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "jump jump jump\nlie\nstrang\n"
     ]
    }
   ]
  },
  {
   "source": [
    "兰卡斯特词干提取器(Lancaster stemmer)基于兰卡斯特词干算法，通常也称为佩斯/哈斯科司干提取器(Paice/Husk stemmer ),由克里斯・D・佩斯(Chris D. Paice )提出。该词干提取器是一个迭代提取器，具有超过 120 条规则来具体说明如何删减或替换词缀以获得词干。以下代码段显示了兰卡斯特词干提取器的用法:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "jump jump jump\nlying\nstrange\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "ls = LancasterStemmer()\n",
    "print(ls.stem('jumping'),ls.stem('jumps'),ls.stem('jumped'))\n",
    "print(ls.stem('lying'))\n",
    "print(ls.stem('strange'))"
   ]
  },
  {
   "source": [
    "## 2.9 词形还原"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "词形还原（lemmatization）的过程与词干提取非常相似，去除词缀以获得单词的基本形式。但在这种情况下，这种基本形式称为根词（root word），而不是词干。它们的不同之处在于，词干不一定是标准的、正确的单词。也就是说，它可能不存在于词典中。根词也称为词元（lemma），始终存在于词典中。 词形还原的过程比词干提取慢很多，因为它涉及一个附加步骤，当且仅当该词元存在于词典中时，才通过去除词缀形成根形式或词元。n1tk 包有一个强大的词形还原模块，它使\n",
    "用 WordNet、单词的句法和语义（如词性和语境）来获得根词或词元。还记得第 1 章的词性吗? 它主要包含三个实体一名词、动词和形容词一最常见于自然语言。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "car\nmen\nrun\neat\nsad\nfancy\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "# 还原名词\n",
    "print(wnl.lemmatize('cars','n'))\n",
    "print(wnl.lemmatize('men','n'))\n",
    "# 还原动词\n",
    "print(wnl.lemmatize('running','v'))\n",
    "print(wnl.lemmatize('ate','v'))\n",
    "# 还原形容词\n",
    "print(wnl.lemmatize('saddest','a'))\n",
    "print(wnl.lemmatize('fancier','a'))"
   ]
  },
  {
   "source": [
    "上述代码段展示了每个单词是如何使用词形还原变回其基本格式的。词形还原有助于我们进行词语标准化。上述代码利用了 WordNetLemmatizer 类，它使用 WordNetCorpusReader 类的 morphy ()函数。该函数使用单词及其词性，通过比对 WordNet 语料库，并采用 递归技术删除词缀直到在词汇网络中找到匹配项，最终获得输入词的基本形式或词元。如果没有找到匹配项，则将返回输入词（输人词不做任何变化）。 在这里，词性非常重要，因为如果词性是错误的，那么词形还原就会失效，如下面的代\n",
    "码段所示 :"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ate\nfancier\n"
     ]
    }
   ],
   "source": [
    "print(wnl.lemmatize('ate','n'))\n",
    "print(wnl.lemmatize('fancier','v'))"
   ]
  },
  {
   "source": [
    "# 3.理解文本句法和结构"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3.1 词性标注(POS)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "词性 $(\\mathrm{POS})$ 是基于语法语境和词语作用的具体词汇分类。第 1 章介绍了 $\\mathrm{POS}$ 的一些基础，并提到了主要的$(\\mathrm{POS})$，包括名词、动词、形容词和副词。对单词进行分类并标记 $(\\mathrm{POS})$ 标签称为词性标注或 POS 标注。$(\\mathrm{POS})$ 标签用于标注单词并描述其词性，当我们需要在基于 NPL的程序中使用注释文本时，这是非常有用的，因为我们可以通过特定的词性过滤数据并利用该信息来执行具体的分析，例如将词汇范围缩小至名词，分析哪些是最突出的词语，消除歧义并进行语法分析。 我们将使用 Penn Treebank 进行 POS 标注。你可以在 www.cis.uni-muenchen.de/schmid/tools/TreeTagger/data/Penn-Treebank-Tagset.pdf 中找到有关各种 POS 标签及其标注的 更多信息，其上包含了详细的说明文档，举例说明了每一项标签。Penn Treebank 项目是宾夕法尼亚大学的一个项目，该项目网站 www.cis.upenn.edu/提供了更多的相关信息。目前， 有各种各样的标签以满足不同的应用场景，例如 POS 标签是分配给单词标记词性的标签， 语块标签通常是分配给短语的标签，还一些标签是用于描述关系的次级标签。下表给出了 词性标签的详细描述及其示例，如果你并不想费力气去查看 Penn Treebank 标签的详细文档， 你可以随时用它作为参考，以便更好地了解 POS 标签和分析树。                       \n",
    "\n",
    "![jupyter](./image/第三章/2.png)\n",
    "![jupyter](./image/第三章/3.png)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Leo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\Leo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Leo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     C:\\Users\\Leo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\Leo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "# 下载pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('wordnet_ic')\n",
    "nltk.download('sentiwordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['The', 'brown', 'fox', 'is', 'quick', 'and', 'he', 'is', 'jumping', 'over', 'the', 'lazy', 'dog']\n\n [('The', 'DET'), ('brown', 'ADJ'), ('fox', 'NOUN'), ('is', 'VERB'), ('quick', 'ADJ'), ('and', 'CONJ'), ('he', 'PRON'), ('is', 'VERB'), ('jumping', 'VERB'), ('over', 'ADP'), ('the', 'DET'), ('lazy', 'ADJ'), ('dog', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "# POS标签器1\n",
    "sentence = 'The brown fox is quick and he is jumping over the lazy dog'\n",
    "\n",
    "import nltk\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tagged_sent = nltk.pos_tag(tokens, tagset='universal')\n",
    "print(tokens)\n",
    "print('\\n',tagged_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('The', 'DT'), ('brown', 'JJ'), ('fox', 'NN'), ('is', 'VBZ'), ('quick', 'JJ'), ('and', 'CC'), ('he', 'PRP'), ('is', 'VBZ'), ('jumping', 'VBG'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# POS标签器2\n",
    "from pattern.en import tag\n",
    "tagged_sent = tag(sentence)\n",
    "print(tagged_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[nltk_data] Downloading package treebank to\n[nltk_data]     C:\\Users\\Leo\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n['The', 'brown', 'fox', 'is', 'quick', 'and', 'he', 'is', 'jumping', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "# POS标签器3：建立自己的POS标签器\n",
    "\"\"\"\n",
    "为了建立自己的标签器，我们需要借助nltk库的treebank语料库的一些数据作为测试集，一些作为训练集训练标签器\n",
    "\"\"\"\n",
    "# 读取treebank语料库\n",
    "from nltk.corpus import treebank\n",
    "data = treebank.tagged_sents()\n",
    "train_data = data[:3500]\n",
    "test_data = data[3500:]\n",
    "print(train_data[0])\n",
    "# 需要验证的例句\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.1454158195372253\n[('The', 'NN'), ('brown', 'NN'), ('fox', 'NN'), ('is', 'NN'), ('quick', 'NN'), ('and', 'NN'), ('he', 'NN'), ('is', 'NN'), ('jumping', 'NN'), ('over', 'NN'), ('the', 'NN'), ('lazy', 'NN'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# 训练方式1：为每个单词都分配相同的POS\n",
    "'''\n",
    "从SequentialBackoffTagger 基类 继承的 DefaultTagger，并为每个单词分配相同的POS\n",
    "'''\n",
    "from nltk.tag import DefaultTagger\n",
    "dt = DefaultTagger('NN')\n",
    "print(dt.evaluate(test_data))   # 测试集的准确率14.5%\n",
    "print(dt.tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.24039113176493368\n[('The', 'NN'), ('brown', 'NN'), ('fox', 'NN'), ('is', 'NNS'), ('quick', 'NN'), ('and', 'NN'), ('he', 'NN'), ('is', 'NNS'), ('jumping', 'VBG'), ('over', 'NN'), ('the', 'NN'), ('lazy', 'NN'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# 训练方式2：使用正则表达式和RegexpTagger匹配POS\n",
    "from nltk.tag import RegexpTagger\n",
    "\n",
    "## 定义匹配模式\n",
    "patterns = [\n",
    "    (r'.*ing$','VBG'),(r'.*ed$','VBD'),(r'.*es$','VBZ'),(r'.*ould$','MD'),(r'.*\\'s$','NN$'),(r'.*s$','NNS'),(r'^-?[0-9]+(.[0-9]+)?$',\"CD\"),(r'.*','NN')\n",
    "]\n",
    "rt = RegexpTagger(patterns)\n",
    "print(rt.evaluate(test_data))   # 准确率24%\n",
    "print(rt.tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.8607803272340013\n",
      "[('The', 'DT'), ('brown', None), ('fox', None), ('is', 'VBZ'), ('quick', 'JJ'), ('and', 'CC'), ('he', 'PRP'), ('is', 'VBZ'), ('jumping', 'VBG'), ('over', 'IN'), ('the', 'DT'), ('lazy', None), ('dog', None)]\n",
      "0.13466937748087907\n",
      "[('The', 'DT'), ('brown', None), ('fox', None), ('is', None), ('quick', None), ('and', None), ('he', None), ('is', None), ('jumping', None), ('over', None), ('the', None), ('lazy', None), ('dog', None)]\n",
      "0.08064672281924679\n",
      "[('The', 'DT'), ('brown', None), ('fox', None), ('is', None), ('quick', None), ('and', None), ('he', None), ('is', None), ('jumping', None), ('over', None), ('the', None), ('lazy', None), ('dog', None)]\n"
     ]
    }
   ],
   "source": [
    "# 训练方式3：n元模型(1元，2元，3元)\n",
    "from nltk.tag import UnigramTagger  # 1元\n",
    "from nltk.tag import BigramTagger  # 2元\n",
    "from nltk.tag import TrigramTagger   # 3元\n",
    "\n",
    "ut = UnigramTagger(train_data)\n",
    "bt = BigramTagger(train_data)\n",
    "tt = TrigramTagger(train_data)\n",
    "\n",
    "print(ut.evaluate(test_data))  # 86.1%准确率\n",
    "print(ut.tag(tokens))\n",
    "print(bt.evaluate(test_data))  # 13.4%准确率\n",
    "print(bt.tag(tokens))\n",
    "print(tt.evaluate(test_data))   # 8%准确率\n",
    "print(tt.tag(tokens))"
   ]
  },
  {
   "source": [
    "标签None表示无法标记该单词，因为他在训练数据中未能找到类似的标记。二元和三元分词模型的准确率远不及一元分词模型，因为在训练集中观测到的二元和三元词组不一定在测试集中以相同方式出现。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**N元模型：**                           \n",
    "http://www.nltk.org/book/ch05.html                           \n",
    "先来介绍词性标注任务是什么：一般来说，在词性标注的问题中，涉及词性之间的转换以及由词性引出单词的概率，例如：词性之间的转换就是当第i个词为名词时第i+1个词性是动词的概率，显然第i+1的词性为动词的概率大于形容词的概率；再者，词性引出单词的概率就是当第i个状态为名词时，它引出book的概率要大于它引出的单词为beautiful的概率。但是我们只能看到这组词的序列是什么，我们并不能看到引出这组词的状态序列，我们需要从这组词的序列去推断引出这组词序列的状态序列！                       \n",
    "再来介绍N元模型是什么：设有一个状态序列，N元语法认为，其中某个状态的是否出现 只与它前面的 N-1 个状态有关。设词序列(句子)为：                      \n",
    "$$\n",
    "\\mathbf{W}=\\mathbf{w}_{\\mathbf{1}}, \\mathbf{w}_{\\mathbf{2}}, \\ldots, \\mathbf{w}_{\\mathbf{n}-\\mathbf{1}}, \\mathbf{w}_{\\mathbf{n}}\n",
    "$$                  \n",
    "一个词序列(句子)的概率等于这些单词的联合概率，即：                \n",
    "若N = 1时：                   \n",
    "$$\n",
    "\\mathbf{P}\\left(\\mathbf{w}_{1}, \\mathbf{w}_{2}, \\ldots, \\mathbf{w}_{\\mathbf{n}}\\right)=\\mathbf{P}\\left(\\mathbf{w}_{1}\\right) \\mathbf{P}\\left(\\mathbf{w}_{2}\\right) \\ldots \\mathbf{P}\\left(\\mathbf{w}_{\\mathbf{n}}\\right)\n",
    "$$                        \n",
    "若N = 2时：                 \n",
    "$$\n",
    "\\begin{array}{c}\n",
    "\\mathbf{P}\\left(\\mathbf{w}_{\\mathbf{1}}, \\mathbf{w}_{2}, \\ldots, \\mathbf{w}_{\\mathbf{n}}\\right)=\\mathbf{P}\\left(\\mathbf{w}_{\\mathbf{1}}\\right) \\mathbf{P}\\left(\\mathbf{w}_{\\mathbf{2}} \\mid \\mathbf{w}_{\\mathbf{1}}\\right) \\ldots \\mathbf{P}\\left(\\mathbf{w}_{\\mathbf{n}} \\mid \\mathbf{w}_{\\mathbf{n}-\\mathbf{1}}\\right) \\\\\n",
    "P\\left(w_{1}, w_{2}, \\ldots, w_{n}\\right)=P\\left(w_{1}\\right) \\prod_{i=2}^{n} P\\left(w_{i} \\mid w_{i-1}\\right)\n",
    "\\end{array}\n",
    "$$                     \n",
    "若N = 3时：                   \n",
    "$$\n",
    "P\\left(w_{1}, w_{2}, \\ldots, w_{n}\\right)=P\\left(w_{1}\\right) P\\left(w_{2} \\mid w_{1}\\right) \\prod_{i=3}^{n} P\\left(w_{i} \\mid w_{i-2}, w_{i-1}\\right)\n",
    "$$                    \n",
    "总结一下：我们要是用N元模型的方式去完成词性标注！下面来看看如何实现用N元模型的方式去词性标注，我们使用一元模型做例子：                   \n",
    "                      \n",
    "![jupyter](./image/第三章/1.jpg)\n",
    "\n",
    "上图中，$z_1,...,z_T$代表词性，这个是不能被观测到的，我们成为隐含状态；$y_1,...,y_T$代表具体分词后的句子，是我们实际观测到的，我们称为观测状态。            \n",
    "隐马尔可夫模型参数： $\\theta=(\\mathbf{A}, \\mathbf{B}, \\pi),$ 通过统计语料库得到                            \n",
    "   - 初始概率： $\\pi$                   \n",
    "   - 状态转移矩阵：A                    \n",
    "   - 发射矩阵：B                 \n",
    "\n",
    "我们的的目标是使用极大似然估计求得:$argmax_{\\theta=(\\mathbf{A}, \\mathbf{B}, \\pi)} \\; P(x,z) $               \n",
    "使用维特比算法：               \n",
    "维特比网格（viterbi trellis）：$m \\times n$矩阵，每个格点记录当前最优路径分值(the score of the best path ending at state j  at time k ):$\\delta_{k}(j)$,即：                         \n",
    "$$\n",
    "\\delta_{k}(j)=\\left\\{\\begin{array}{ll}\n",
    "\\log P\\left(y_{k} \\mid z_{k}=j\\right)+\\max _{i=1, \\ldots, m}\\left(\\delta_{k-1}\\left(z_{k-1}=i\\right)+\\log P\\left(z_{k}=j \\mid z_{k-1}=i\\right)\\right), & k>1 \\\\\n",
    "\\log P\\left(y_{1} \\mid z_{1}=j\\right)+\\log P\\left(z_{1}=j\\right), & k=1\n",
    "\\end{array}\\right.\n",
    "$$                 \n",
    "即：                 \n",
    "$$\n",
    "\\delta_{k}(j)=\\left\\{\\begin{array}{ll}\n",
    "B_{j, y_{k}}+\\max _{i=1, \\ldots, m}\\left(\\delta_{k-1}(i)+A_{i, j}\\right), & k>1 \\\\\n",
    "B_{j, y_{1}}+\\pi_{j}, & k=1\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "                         \n",
    "![jupyter](./image/第三章/4.png)                \n",
    "\n",
    "我们从后往前依次寻找最优路径，如上图就能找到最佳的隐含序列啦！！！"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9094781682641108\n[('The', 'DT'), ('brown', 'NN'), ('fox', 'NN'), ('is', 'VBZ'), ('quick', 'JJ'), ('and', 'CC'), ('he', 'PRP'), ('is', 'VBZ'), ('jumping', 'VBG'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'NN'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# 训练方式4：组合所有的组合标签器\n",
    "def combined_tagger(train_data,taggers,backoff = None):\n",
    "    for tagger in taggers:\n",
    "        backoff = tagger(train_data,backoff=backoff)\n",
    "    return backoff\n",
    "\n",
    "ct = combined_tagger(train_data=train_data,taggers=[UnigramTagger,BigramTagger,TrigramTagger],backoff=rt)\n",
    "print(ct.evaluate(test_data)) # 91%的准确率\n",
    "print(ct.tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9306806079969019\n[('The', 'DT'), ('brown', 'JJ'), ('fox', 'NN'), ('is', 'VBZ'), ('quick', 'JJ'), ('and', 'CC'), ('he', 'PRP'), ('is', 'VBZ'), ('jumping', 'VBG'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'VBG')]\n"
     ]
    }
   ],
   "source": [
    "# 训练方式5：使用有监督学习的分类算法\n",
    "from nltk.classify import NaiveBayesClassifier  # 朴素贝叶斯分类器\n",
    "from nltk.tag.sequential import ClassifierBasedPOSTagger\n",
    "\n",
    "nbt = ClassifierBasedPOSTagger(train = train_data,classifier_builder=NaiveBayesClassifier.train)\n",
    "\n",
    "print(nbt.evaluate(test_data))  # 准确率93%\n",
    "print(nbt.tag(tokens))"
   ]
  },
  {
   "source": [
    "## 3.2 浅层分析"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "浅层分析（ shallow parsing）也称为浅分析（light parsing）或组块分析（chunking）, 是分析句子结构的一种技术，它将句子分解成最小的组成部分（它们是标识，如单词），然后将它们组合成更高级的短语。在浅层分析中，主要的关注焦点是识别这些短语或语块，而不是挖掘每个块内句法和语句关系的深层细节，正如我们在基于深度分析获得的分析树中看到的。浅层分析的主要目的是获得语义上有意义的短语，并观察它们之间的关系。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['The', 'DT', 'B-NP', 'O', 'NP-SBJ-1'], ['brown', 'JJ', 'I-NP', 'O', 'NP-SBJ-1'], ['fox', 'NN', 'I-NP', 'O', 'NP-SBJ-1'], ['is', 'VBZ', 'B-VP', 'O', 'VP-1'], ['quick', 'JJ', 'B-ADJP', 'O', 'O'], ['and', 'CC', 'O', 'O', 'O'], ['he', 'PRP', 'B-NP', 'O', 'NP-SBJ-2'], ['is', 'VBZ', 'B-VP', 'O', 'VP-2'], ['jumping', 'VBG', 'I-VP', 'O', 'VP-2'], ['over', 'IN', 'B-PP', 'B-PNP', 'O'], ['the', 'DT', 'B-NP', 'I-PNP', 'O'], ['lazy', 'JJ', 'I-NP', 'I-PNP', 'O'], ['dog', 'NN', 'I-NP', 'I-PNP', 'O']]\n"
     ]
    }
   ],
   "source": [
    "# 浅层分析器1：pattern包创建\n",
    "sentence = 'The brown fox is quick and he is jumping over the lazy dog'\n",
    "\n",
    "from pattern.en import parse\n",
    "tree = parse(sentence,relations=True,lemmate=True)\n",
    "\n",
    "print(tree.split()[0])"
   ]
  },
  {
   "source": [
    "# 浅层分析器2：基于训练的模式\n",
    "from nltk.corpus import treebank_chunk\n",
    "\n",
    "# 分割来自treebank_chunk的数据(已经带标签)\n",
    "data = treebank_chunk.chunked_sents()\n",
    "train_data = data[:4000]\n",
    "test_data = data[4000:]\n",
    "print(train_data[7]) # 可以看出数据点由短语和POS标签完成的标注\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 86,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(S\n  (NP A/DT Lorillard/NNP spokewoman/NN)\n  said/VBD\n  ,/,\n  ``/``\n  (NP This/DT)\n  is/VBZ\n  (NP an/DT old/JJ story/NN)\n  ./.)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(S\n  (NP A/DT Lorillard/NNP spokewoman/NN)\n  said/VBD\n  ,/,\n  ``/``\n  (NP This/DT)\n  is/VBZ\n  (NP an/DT old/JJ story/NN)\n  ./.)\n\n [('A', 'DT', 'B-NP'), ('Lorillard', 'NNP', 'I-NP'), ('spokewoman', 'NN', 'I-NP'), ('said', 'VBD', 'O'), (',', ',', 'O'), ('``', '``', 'O'), ('This', 'DT', 'B-NP'), ('is', 'VBZ', 'O'), ('an', 'DT', 'B-NP'), ('old', 'JJ', 'I-NP'), ('story', 'NN', 'I-NP'), ('.', '.', 'O')]\n\n (S\n  (NP A/DT Lorillard/NNP spokewoman/NN)\n  said/VBD\n  ,/,\n  ``/``\n  (NP This/DT)\n  is/VBZ\n  (NP an/DT old/JJ story/NN)\n  ./.)\n"
     ]
    }
   ],
   "source": [
    "## 尝试使用tree2colltags函数和conlltags2tree函数\n",
    "from nltk.chunk.util import tree2conlltags,conlltags2tree\n",
    "\n",
    "train_sent = train_data[7]\n",
    "print(train_sent)\n",
    "\n",
    "wtc = tree2conlltags(train_sent)\n",
    "print(\"\\n\",wtc)\n",
    "\n",
    "tree = conlltags2tree(wtc)\n",
    "print(\"\\n\",tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 定义一个函数conll_tag_chunks()从分块标注好的句子提取POS和块标签\n",
    "def conll_tag_chunks(chunk_sents):\n",
    "    tagged_sent = [tree2conlltags(tree) for tree in chunk_sents]\n",
    "    return [[(t,c) for (w,t,c) in sent] for sent in tagged_sent]\n",
    "\n",
    "def combined_tagger(train_data,taggers,backoff = None):\n",
    "    for tagger in taggers:\n",
    "        backoff = tagger(train_data,backoff=backoff)\n",
    "    return backoff\n",
    "\n",
    "## 输入一个标记好的三元标签的组，单词、pos、块：\n",
    "from nltk.tag import UnigramTagger,BigramTagger\n",
    "from nltk.chunk import ChunkParserI\n",
    "\n",
    "class NGramTagChunker(ChunkParserI):\n",
    "    def __init__(self,train_sentences,tagger_classes=[UnigramTagger,BigramTagger]):\n",
    "        train_sent_tags = conll_tag_chunks(train_sentences)\n",
    "        self.chunk_tagger = combined_tagger(train_sent_tags,tagger_classes)\n",
    "\n",
    "    "
   ]
  }
 ]
}