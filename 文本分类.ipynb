{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "d9d363e889bac6e082d872d6c6a952a67371d63acf869877e6b992428fa21dbf"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 引言"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "文本分类或归类涉及基于每个文件的内在属性或特性，尽量将文本文件划分为不同的类别。该技术可用在不同的领域，包括垃圾邮件识别和新闻分类。如果你只有少量的文档，你可以查看每个文档，知道其内容，尽量去标识它们， 分类这个概念也许就比较简单。基于这些知识，你可以将相似的文档归到不同的类型。当需要分类的文档数量增加到几十万或百万时，这将更具挑战。像特征提取、有监督学习和无监督学习这些技术都能派上用场。文档分类是一个通用的问题，不仅仅局限在文本，还可以拓展到其他方面，像音乐、图像、视频和其他媒体。 为了更清楚地把问题形式化，我们将使用一组给定的类或类别及若干文本文件。请记住，文件基本上由句子或段落文本组成。这就形成了一个语料库。我们的任务是确定每个文档属于哪一类或哪些类。整个过程包括几个步骤，这将在本章后面详细讨论。简言之，对于有监督分类问题，我们需要一些标记的数据用来训练文本分类模型。这些数据基本上是组织好的文件，已经预先分配给某些特定的类或类别。使用这些数据，我们可以从每个文档提取特征和属性，通过将这些数据送入一个有监督学习算法，使模型学习这些与每个文档对应的属性和分类或类别。当然，在建立模型前，数据需要预处理和规范化处理。一旦处理完毕， 我们将对新文档采用同样的规范化处理和特征提取方法，然后将这些特征送入模型，预测这些文档的分类或类型。然而，对于无监督学习问题，我们基本上没有预先标注好的训练文档。基于文档内在的特性，我们将使用像聚类、文档相似性度量技术实现文档聚类，并为文档分配类型。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 1.什么是文本分类"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "假设有一个预定义的类集合，文本或文档分类是将文档指定到一个或多个分类或类型的过程。这里的文档就是文本文档，每个文档包含单词组成的句子或段落。一个文本分类系统基于文档的内置属性，能够成功地将每个文档分类到正确的类别中。数学上，可以做如下定义: 假设 $d$ 是文档 $D$ 的描述或属性， $d \\in D$ ，我们具有一组预先定义的类别或分类 $C=\\left\\{c_{1}\\right.$, $\\left.c_{2}, c_{3}, \\cdots, c_{n}\\right\\} \\circ$ 真实的文档 $D$ 可能拥有很多内在的属性，这使得 $D$ 成为高维空间的一个实体。使用这个空间的一个子集，其是包含一组有限的描述或特征的集合，表示为 $d,$ 可以使 用文本分类系统 $T$ 成功地将原始文档 $D$ 划分到正确的类型 $C_{x \\circ}$ 这可以表示为 $T: D \\rightarrow C_{x \\circ}$             \n",
    "\n",
    "![jupyter](./image/第四章/1.png)\n",
    "\n",
    "文本分类有以下两种方法：                \n",
    "   - 基于内容的分类\n",
    "   - 基于请求的分类\n",
    "\n",
    "这两类的差异在于文本文档分类方法背后的思想或理念，而不在于具体的技术算法与过程。基于内容的分类是根据文本内容主题或题目的属性或权重来进行文档分类的。举一个概念性的例子，一本书有 $30 \\%$ 以上的内容是关于食物准备的，这本书可以归为烹饪/识谱类。基于请求的分类受到用户需求的影响，其目标是特定的用户群和读者。这类分类受到特\n",
    "殊策略和思想的控制。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 2.如何使用机器学习对文本分类"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "无监督学习指的是不需要提前标注训练数据样本来建立模型的具体的机器学习技术或算法。通常，有一个数据点集合，它可以是文本或数字类型的，这取决于要解决的具体问题。我们通过名为“特征提取”的过程从每个数据中提取特征，然后将来自于每个数据的特征集合输入算法。我们尽力从这些数据中提取有意义的模式，例如使用聚类或基于主题模型的文本摘要技术对相似的数据进行分组。这项技术在文本分类中非常有用的，也称为文档聚类，即我们仅仅依靠文档的特征、相似度和属性，而不需要使用标注数据训练任何模型进行文档分组。后续的章节将进一步讨论无监督学习，包括主题建模、文档摘要、相似性分析和聚类。                   \n",
    "有监督学习指的是训练预标注数据样本（也称为训练数据）的具体机器学习技术或算法。使用特征提取从数据中提取特征或属性，对于每个数据点，我们将拥有特征集和对应的类型/标签。算法从训练数据中学习每个分类的不同模式。学习完成后，我们得到一个训练好的模型。一旦我们将未来测试数据样本的特征送入这个模型，模型就可以预测这些测试数据样本的分类。这样机器就学会了如何基于训练的数据样本预测未知的新数据样本的分类。                         \n",
    "现在，我们已经准备好从数学上对自动基于机器的文本分类过程进行定义。有一个文档集合，集合中文档带有相应的类别或分类标签。这个集合可以用 $T S$ 表示，这是一个文档和标签对的集合, $T S=\\left\\{\\left(d_{1}, c_{1}\\right),\\left(d_{2}, c_{2}\\right), \\cdots,\\left(d_{n}, c_{n}\\right)\\right\\}$，其中 $d_{1}, d_{2}, \\cdots, d_{n}$ 是文本列表，$c_{1}, c_{2}, \\cdots, c_{n}$ 是这些文本对应的类型。这里 $\\left.c_{x} \\in C=\\mid c_{1}, c_{2}, \\cdots, c_{n}\\right\\}$，其中 $c_{x}$ 表示文档 $x$对应的类型， $C$ 表示所有可能离散分类的集合，集合中任何元素可能是文档的一个或多个类型。假设我们已经拥有了训练数据集，我们可以定义一个有监督学习算法 $F,$ 当算法在训练数据 $T S$ 集上训练之后，我们得到训练好的分类器 $\\gamma$，可以表示为 $F(T S)=\\gamma_{\\circ}$ 因此，有监督学习算法 $F$ 使用输入集（ document，class ）对 $T S$，得到训练的分类器 $\\gamma \\longrightarrow$ 这就是我们的模型。上述过程就称之为训练过程。\n",
    "这个模型输入一个新的、未知的文档 $N D,$ 可以预测文档的类型 $c_{N D}$，使得 $c_{N D} \\in C_{\\circ}$ 这一过程称为预测过程，可以表示为 $\\gamma: T D \\rightarrow c_{N D}$，这样我们看到有监恪文本分类过程有两个主要的过程 :\n",
    "   - 训练\n",
    "   - 预测\n",
    "\n",
    "为了得到机器学习文本分类系统，我们需要在获取数据后做如下操作： (详情可以看另一个文档《集成学习》)              \n",
    "   - 准备训练和测试数据\n",
    "   - 文本规范化处理\n",
    "   - 特征抽取\n",
    "   - 模型训练\n",
    "   - 模型评估与超参数调优\n",
    "   - 预测和模型部署             \n",
    "\n",
    "![jupyter](./image/第四章/2.png)\n",
    "\n",
    "   "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 3.文本规范化处理"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "由于在《处理和理解文本》中对处理文本部分进行了详细的介绍，这里仅用代码的形式说明文本规范化的步骤：\n",
    "   - 扩展停用词\n",
    "   - 通过词形还原\n",
    "   - 去除特殊字符和符号\n",
    "   - 去除停用词"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引用必要的工具库\n",
    "from contractions import CONTRACTION_MAP  # 储藏大量的扩展停用词\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer   # 实现词形还原\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词语分词\n",
    "def tokenize_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]           # 去除分词后的多余符号\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义扩展停用词的函数\n",
    "def expand_contractions(text, contraction_mapping):\n",
    "    '''\n",
    "    |：代表or，用于匹配前一个或者后一个正则表达式\n",
    "    flags参数：匹配的模式\n",
    "    re.IGNORECASE：忽略大小写\n",
    "    re.DOTALL：默认情况下，正则表达式中的dot（.），表示所有除了换行的字符，加上re.DOTALL参数后，就是真正的所有字符了，包括换行符（\\n）\n",
    "    '''\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),flags=re.IGNORECASE|re.DOTALL)   \n",
    "    \n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        \"\"\"\n",
    "        group(0) :group() 同group（0）就是匹配正则表达式整体结果,group(1) 列出第一个括号匹配部分，group(2) 列出第二个括号匹配部分，group(3) 列出第三个括号匹配部分。\n",
    "        get(key):返回指定键的值，如果键不在字典中返回默认值 None 或者设置的默认值。\n",
    "\n",
    "        \"\"\"\n",
    "        first_char = match[0]\n",
    "        expand_contraction = contraction_mapping.get(match) if contraction_mapping.get(match) else contraction_mapping.get(match.lower())\n",
    "        expand_contraction = first_char + expand_contraction[1:]\n",
    "        return expand_contraction\n",
    "\n",
    "    expand_text = contractions_pattern.sub(expand_match, text)\n",
    "    expand_text = re.sub(\"'\",\"\",expand_text)\n",
    "    return expand_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词形还原\n",
    "from pattern.en import tag\n",
    "from nltk.corpus import wordnet as wn \n",
    "\n",
    "# 获得文本和文本的标签\n",
    "def pos_tag_text(text):\n",
    "    # 转换penn treebank 到 pos标签\n",
    "    def penn_to_wn_tags(pos_tag):\n",
    "        if pos_tag.startswith('J'):\n",
    "            return wn.ADJ\n",
    "        elif pos_tag.startswith('V'):\n",
    "            return wn.VERB\n",
    "        elif pos_tag.startswith('N'):\n",
    "            return wn.NOUN\n",
    "        elif pos_tag.startswith('R'):\n",
    "            return wn.ADV\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    tagged_text = tag(text)\n",
    "    tagged_lower_text = [(word.lower(),penn_to_wn_tags(pos_tag=pos_tag)) for word,pos_tag in tagged_text]\n",
    "    return tagged_lower_text\n",
    "\n",
    "# 基于pos标签的词形还原\n",
    "def lemmatize_text(text):\n",
    "    pos_tagged_text = pos_tag_text(text)\n",
    "    lemmatized_tokens = [wnl.lemmatize(word,pos_tag) if pos_tag else word for word,pos_tag in pos_tagged_text]\n",
    "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特殊字符和符号的去除\n",
    "def remove_special_characters(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_tokens = filter(None,[pattern.sub('',token) for token in tokens])\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "# 去除停用词\n",
    "def remove_stopwords(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将文本规范化函数组合形成流水线\n",
    "def normalize_corpus(corpus,tokenize = False):\n",
    "    normalized_corpus = []\n",
    "    for text in corpus:\n",
    "        text = expand_contractions(text,CONTRACTION_MAP)\n",
    "        text = lemmatize_text(text)\n",
    "        text = remove_special_characters(text)\n",
    "        text = remove_stopwords(text)\n",
    "        normalized_corpus.append(text)\n",
    "        if tokenize:\n",
    "            text = tokenize_text(text)\n",
    "            normalized_corpus.append(text)\n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['sky blue',\n",
       " 'sky blue sky beautiful',\n",
       " 'beautiful sky blue',\n",
       " 'love blue cheese']"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# 文档内容\n",
    "CORPUS = [\n",
    "    'the sky is blue',\n",
    "    'sky is blue and sky is beautiful',\n",
    "    'the beautiful sky is blue',\n",
    "    'i love blue cheese'\n",
    "]\n",
    "normalize_corpus(CORPUS)"
   ]
  },
  {
   "source": [
    "# 4.特征提取"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "向量空间模型是处理文本数据非常有用的概念和模型，并在信息检索与文档排序中广泛使用。向量空间模型也称为词向量模型，定义为文本文档转换与表示的数学或数模型，作为形成向量维度的特定词项的数字向量。数学上定义如下，假设在文档向量空间 $V S$ 中有一个文档D，每个文档维度和列数量将是向量空间中全部文档中不同词项或单词的总数量。 因此，向量空间可以表示为：\n",
    "$$\n",
    "V S=\\left\\{W_{1}, W_{2}, \\cdots, W_{n}\\right\\}\n",
    "$$\n",
    "其中，n是全部文档中不同单词的数量。现在，可以把文档D在向量空间表示为：                            \n",
    "$$\n",
    "D=\\left\\{w_{D_{1}}, w_{D_{2}}, \\cdots, w_{D_{n}} \\mid\\right.\n",
    "$$\n",
    "其中， $w_{D_{n}}$ 表示文档D中第n个词的权重。这个权重是一个数量值，可以表示任何事，可以是文档中单词的频率、平均的出现频率，或者是 TF-IDF 权重（稍后介绍）。 下面将介绍和实现如下特征提取技术:\n",
    "   - 词袋模型。\n",
    "   - TF-IDF 模型。\n",
    "   - 高级词向量模型。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练文本与测试文本\n",
    "CORPUS = [\n",
    "    'the sky is blue',\n",
    "    'sky is blue and sky is beautiful',\n",
    "    'the beautiful sky is so blue',\n",
    "    'i love blue cheese'\n",
    "]\n",
    "new_doc = ['loving this blue sky today']"
   ]
  },
  {
   "source": [
    "## 4.1 词袋模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "词袋模型也许是从文本文档中提取特征最简单但又最有效的技术。这个模型的本质是将文本文档转化成向量，从而将每个文档转化成一个向量，这个向量表示在文档空间中全部不同的单词在该文档中出现的频率。因此，根据前面的数学定义，这里的例子向量记为D, 每个单词的权重与该词在文档中出现的频率相等。下面举个例子说明：                   \n",
    "例句:                       \n",
    "   - Jane wants to go to Shenzhen.\n",
    "   - Bob  wants to go to Shanghai.\n",
    "\n",
    "将所有词语装进一个袋子里，不考虑其词法和语序的问题，即每个词语都是独立的。例如上面2个例句，就可以构成一个词袋，袋子里包括Jane、wants、to、go、Shenzhen、Bob、Shanghai。假设建立一个数组（或词典）用于映射匹配：               \n",
    "$$\n",
    "[Jane, wants, to, go, Shenzhen, Bob, Shanghai]\n",
    "$$                         \n",
    "那么上面两个例句就可以用以下两个向量表示，对应的下标与映射数组的下标相匹配，其值为该词语出现的次数:\n",
    "$$\n",
    "[1,1,2,1,1,0,0]\\\\\n",
    "[0,1,2,1,0,1,1]\n",
    "$$\n",
    "这两个词频向量就是词袋模型，可以很明显的看到语序关系已经完全丢失。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词袋模型\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def bow_extractor(corpus,ngram_range=(1,1)):\n",
    "    '''\n",
    "    min_df:整个文档空间中最小频率为min_df的词都会考虑\n",
    "    ngram_range:All values of n suchsuch that min_n <= n <= max_n will be used. For example anngram_range of (1, 1) means only unigrams, (1, 2) meansunigrams and bigrams, and (2, 2) means only bigrams.\n",
    "    '''\n",
    "    vectorizer = CountVectorizer(min_df=1,ngram_range=ngram_range)\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer,features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0 0 1 0 1 0 1 0 1]\n [1 1 1 0 2 0 2 0 0]\n [0 1 1 0 1 0 1 1 1]\n [0 0 1 1 0 1 0 0 0]]\n\n [[0 0 1 0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# 打印训练集的编码结果\n",
    "bow_vectorizer,bow_features = bow_extractor(CORPUS)\n",
    "features = bow_features.todense()\n",
    "print(features)\n",
    "# 打印测试集的编码结果\n",
    "new_doc_features = bow_vectorizer.transform(new_doc)\n",
    "new_doc_features = new_doc_features.todense()\n",
    "print(\"\\n\",new_doc_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['and', 'beautiful', 'blue', 'cheese', 'is', 'love', 'sky', 'so', 'the']\n"
     ]
    }
   ],
   "source": [
    "# 打印特征名称\n",
    "feature_names = bow_vectorizer.get_feature_names()\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   and  beautiful  blue  cheese  is  love  sky  so  the\n",
       "0    0          0     1       0   1     0    1   0    1\n",
       "1    1          1     1       0   2     0    2   0    0\n",
       "2    0          1     1       0   1     0    1   1    1\n",
       "3    0          0     1       1   0     1    0   0    0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>and</th>\n      <th>beautiful</th>\n      <th>blue</th>\n      <th>cheese</th>\n      <th>is</th>\n      <th>love</th>\n      <th>sky</th>\n      <th>so</th>\n      <th>the</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "# 清晰展示编码结果\n",
    "import pandas as pd \n",
    "pd.DataFrame(data=features,columns = feature_names)"
   ]
  },
  {
   "source": [
    "请注意，对于新的文档变量 new_doc，这句话中没有 today、this 或 loving 这些单词，因此没有这些词的特征。前面提到过这个原因，就是特征提取过程、模型和词汇总是基于训练数据，将不随着新文档变化或受其影响，这将用于后面的测试或其他语料的预测。你或许已经猜到这是因为一个模型总是基于训练数据进行训练，除非重新建立模型，否则模型不会受到新文档的影响。因此， 这个模型的特征总是受限于训练语料的文档向量空间。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 4.2 TF-IDF模型 "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "词袋模型还不错，但向量完全依赖于单词出现的绝对频率。这存在一些潜在的问题，语料库全部文档中出现次数较多的单词将会拥有较高的频率，这些词将会影响其他一些出现不如这些词频繁但对于文档分类更有意义和有效的单词。举个例子：我们可能发现\"中国\"、\"蜜蜂\"、\"养殖\"这三个词的出现次数一样多。这是不是意味着，作为关键词，它们的重要性是一样的？显然不是这样。因为\"中国\"是很常见的词，相对而言，\"蜜蜂\"和\"养殖\"不那么常见。如果这三个词在一篇文章的出现次数一样多，有理由认为，\"蜜蜂\"和\"养殖\"的重要程度要大于\"中国\"，也就是说，在关键词排序上面，\"蜜蜂\"和\"养殖\"应该排在\"中国\"的前面。所以，我们需要一个重要性调整系数，衡量一个词是不是常见词。**如果某个词比较少见，但是它在这篇文章中多次出现，那么它很可能就反映了这篇文章的特性，正是我们所需要的关键词。** 因此，从算法设计的角度来说：用统计学语言表达，就是在词频的基础上，要对每个词分配一个\"重要性\"权重。最常见的词（\"的\"、\"是\"、\"在\"）给予最小的权重，因为这些词对语义来说没什么贡献，甚至可以删除，现实中也是常常删除这些词；较常见的词（\"中国\"）给予较小的权重，较少见的词（\"蜜蜂\"、\"养殖\"）给予较大的权重。这个权重叫做\"逆文档频率\"（Inverse Document Frequency，缩写为IDF），它的大小与一个词的常见程度成反比。                             \n",
    "现在正式定义 TF-IDF，开始实现之前，看一下它的数学表示。数学上，TF-IDF 是两个度量的乘积，可以表示为 $t f i d f=t f \\times i d f,$ 其中词频（ tf) 和逆文档频率（idf）是两个度量。 \n",
    "\n",
    "词频由 tf 表示，由词袋模型计算得出。任何文档的词频是该词在特定文档出现的原始频率值。数学上，词频可以表示为 $t f(w, D)=f_{w_{0}}$, 其中 $f_{w_{0}}$ 表示单词 w 在文档 D 中的频率，这就是词频（ tf) 。有一些其他的词频表示与计算方法，例如将频率转化为二进制频率，其中 1代表单词在文档中出现过，0 则代表没有出现过。有时，也可以通过对数运算或频率平均值将原始频率标准化。我们将在具体实现中使用原始频率。说白了，词频tf就是某个词在文章出现的次数除以文章的单词总数。           \n",
    "\n",
    "逆文档频率由 idf 表示，是每个单词的文档频率的逆。该值由语料库中全部文档数量除以每个单词的文档频率，然后对结果应用对数运算变换其比例。在这里的实现中，将对每个单词的文档频率加 1,意味着词汇表中每个单词至少包含在一个语料库文档之中。这是为了避免被 0 除的错误，平滑逆文档频率。也对 idf 的计算结果加1,避免被忽略单词拥有 0 值的 idf。数学上，idf 实现表示如下 :\n",
    "$$\n",
    "i d f(t)=1+\\log \\frac{C}{1+d f(t)}\n",
    "$$\n",
    "其中， $i d f(t)$ 表示单词t的idf, C表示语料库中文档的总数量， $d f(t)$ 表示包含单词t的文档数量频率。 \n",
    "\n",
    "**可以看到，TF-IDF与一个词在文档中的出现次数成正比，与该词在整个语言中的出现次数成反比。**              \n",
    "\n",
    "因此，词频 - 逆文档频率可以通过把两个度量乘在一起来计算。最终将要使用的 TF-IDF 度量是 tfidf 矩阵的归一化版本，矩阵是 tf 和 idf 的乘积。将 tfidf 矩阵除以矩阵的 L2 范数来进行矩阵归一化，L2 范数也称为欧几里得范数，它是每个单词 tfidf 权重平方和的平方根。 数学上，将最终的 tfidf 特征向量表示为 $t f i d f=\\frac{t f i d f}{\\|t f i d f\\|}$, 其中， $||tfidf ||$表示 tfidf 矩阵的欧几里得 L2 范数。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用sklearn实现TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "def tfidf_transformer(bow_matrix):\n",
    "    transformer = TfidfTransformer(norm='l2',smooth_idf=True,use_idf=True)\n",
    "    tfidf_matrix = transformer.fit_transform(bow_matrix)\n",
    "    return transformer,tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
       "0  0.00       0.00  0.40    0.00  0.49  0.00  0.49  0.00  0.60\n",
       "1  0.44       0.35  0.23    0.00  0.56  0.00  0.56  0.00  0.00\n",
       "2  0.00       0.43  0.29    0.00  0.35  0.00  0.35  0.55  0.43\n",
       "3  0.00       0.00  0.35    0.66  0.00  0.66  0.00  0.00  0.00"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>and</th>\n      <th>beautiful</th>\n      <th>blue</th>\n      <th>cheese</th>\n      <th>is</th>\n      <th>love</th>\n      <th>sky</th>\n      <th>so</th>\n      <th>the</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.40</td>\n      <td>0.00</td>\n      <td>0.49</td>\n      <td>0.00</td>\n      <td>0.49</td>\n      <td>0.00</td>\n      <td>0.60</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.44</td>\n      <td>0.35</td>\n      <td>0.23</td>\n      <td>0.00</td>\n      <td>0.56</td>\n      <td>0.00</td>\n      <td>0.56</td>\n      <td>0.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.00</td>\n      <td>0.43</td>\n      <td>0.29</td>\n      <td>0.00</td>\n      <td>0.35</td>\n      <td>0.00</td>\n      <td>0.35</td>\n      <td>0.55</td>\n      <td>0.43</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.35</td>\n      <td>0.66</td>\n      <td>0.00</td>\n      <td>0.66</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "# 查看训练样本的tfidf矩阵\n",
    "import numpy as np \n",
    "\n",
    "bow_vectorizer,bow_features = bow_extractor(CORPUS)\n",
    "feature_names = bow_vectorizer.get_feature_names()\n",
    "tfidf_trans,tfidf_features = tfidf_transformer(bow_features)\n",
    "features = np.round(tfidf_features.todense(),2)\n",
    "pd.DataFrame(data=features,columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   and  beautiful  blue  cheese   is  love   sky   so  the\n",
       "0  0.0        0.0  0.63     0.0  0.0   0.0  0.77  0.0  0.0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>and</th>\n      <th>beautiful</th>\n      <th>blue</th>\n      <th>cheese</th>\n      <th>is</th>\n      <th>love</th>\n      <th>sky</th>\n      <th>so</th>\n      <th>the</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.63</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.77</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "# 查看测试样本的tfidf矩阵\n",
    "\n",
    "new_doc_features = bow_vectorizer.transform(new_doc)\n",
    "nd_tfidf = tfidf_trans.transform(new_doc_features)\n",
    "nd_features = np.round(nd_tfidf.todense(),2)\n",
    "pd.DataFrame(data=nd_features,columns=feature_names)"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 4.3 高级词向量模型word2vec\n",
    "参考博客：https://blog.csdn.net/xbinworld/article/details/90416529"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "先讲一讲one-hot词向量和distributed representation分布式词向量\n",
    "   - one hot的表示形式：word vector $=[0,1,0, \\ldots, 0],$ 其中向量维数为词典的个数|V|，当前词对应的位置为1，其他位置为0。\n",
    "   - distributed的表示形式：word vector = $[0.171,-0.589,-0.346,...,0.863]$, 其中向量维数需要自己指定（比如设定256维等），每个维度的数值需要通过训练学习获得。\n",
    "\n",
    "虽然one-hot词向量构造起来很容易, 但通常并不是一个好选择。一个主要的原因是, one-hot词向量无法准确表达不同词之间的相似度, 如我们常常使用的余弦相似度。对于向量 $x, y \\in R^{d},$ 它们的余弦相似度是它们之间夹角的余弦值\n",
    "$$\n",
    "\\frac{\\boldsymbol{x}^{\\top} \\boldsymbol{y}}{\\|\\boldsymbol{x}\\|\\|\\boldsymbol{y}\\|} \\in[-1,1]\n",
    "$$\n",
    "word2vec工具的提出正是为了解决上面这个问题 。它将每个词表示成一个定长的向量, 并使得这些向量能较好地表达不同词之间的相似和类比关系。word2vec工具包含了两个模型, 即跳字模型 (skip-gram) 和连续词袋模型 (continuous bag of words, CBOW) 。接下来 让我们分别介绍这两个模型以及它们的训练方法。     \n",
    "\n",
    "跳字模型（skip-gram）：\n",
    "\n",
    "![jupyter](./image/第四章/3.png)\n",
    "\n",
    "![jupyter](./image/第四章/4.png)\n",
    "\n",
    "注解：看到这里，就引出了word2vec的核心方法，其实就是认为每个词相互独立，用连乘来估计最大似然函数，求解目标函数就是最大化似然函数。上面公式涉及到一个中心词向量v，以及背景词向量u，因此很有趣的是，可以用一个input-hidden-output的三层神经网络来建模上面的skip-model。\n",
    "\n",
    "![jupyter](./image/第四章/1.jpg)\n",
    "\n",
    "输入的表示：输入层中每个词由独热编码方式表示, 即所有词均表示成一个N维向量, 其中N为词汇表中单词的总数。在向量中, 每个词都将与之对应的维度置为1,其余维度的值均为0。                       \n",
    "网络中传播的前向过程：输出层向量的值可以通过隐含层向量 (K维)，以及连接隐藏层和输出层之间的KxN维权重矩阵计算得到。                   \n",
    "输出层也是一个N维向量, 每维与词汇表中的一个单词相对应。最后对输出层向量应用Softmax激活函数, 可以计算每一个单词的生成概率。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}